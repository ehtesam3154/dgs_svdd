{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1662554863557,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "baRl7aGrjfG7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662554865041,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "TBupr-iA_4KS"
   },
   "outputs": [],
   "source": [
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1662554865518,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "FOQpo4iE_8Gr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1662554866853,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "xFE4QiIzABCd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662555253624,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "SIa4W55Mmlre"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=6,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        # copy pasted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1662555318090,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "mAbeSI5SmoYA"
   },
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a transformer model that can be used for times series\n",
    "    forecasting. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool, \n",
    "        out_seq_len: int=51,\n",
    "        dim_val: int=6, \n",
    "        n_encoder_layers: int=1,\n",
    "        n_decoder_layers: int=1,\n",
    "        n_heads: int=2,\n",
    "        dropout_encoder: float=0.1, \n",
    "        dropout_decoder: float=0.1,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=1024,\n",
    "        dim_feedforward_decoder: int=1024,\n",
    "        num_predicted_features: int=8\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        print(\"input_size is: {}\".format(input_size))\n",
    "        print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=num_predicted_features,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=num_predicted_features\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input, \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input, \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        # print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        # print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        \n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        # print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "        \n",
    "        \"\"\"\n",
    "        THIS IS MY TEMPORAL EMEBBING\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        # print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        # print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        # print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "#         decoder_output = torch.nn.Softmax()(decoder_output) #add activation\n",
    "\n",
    "\n",
    "        return src, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662555319796,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "ctATehFiEDfB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional, Any, Union, Callable, Tuple\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class utils:\n",
    "    def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim1: int, for both src and tgt masking, this must be target sequence\n",
    "              length\n",
    "            dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "        Return:\n",
    "            A Tensor of shape [dim1, dim2]\n",
    "        \"\"\"\n",
    "        return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "    def get_indices_input_target(num_obs, input_len, step_size, forecast_horizon, target_len):\n",
    "        \"\"\"\n",
    "          Produce all the start and end index positions of all sub-sequences.\n",
    "          The indices will be used to split the data into sub-sequences on which \n",
    "          the models will be trained. \n",
    "          Returns a tuple with four elements:\n",
    "          1) The index position of the first element to be included in the input sequence\n",
    "          2) The index position of the last element to be included in the input sequence\n",
    "          3) The index position of the first element to be included in the target sequence\n",
    "          4) The index position of the last element to be included in the target sequence\n",
    "          \n",
    "          Args:\n",
    "              num_obs (int): Number of observations in the entire dataset for which\n",
    "                              indices must be generated.\n",
    "              input_len (int): Length of the input sequence (a sub-sequence of \n",
    "                              of the entire data sequence)\n",
    "              step_size (int): Size of each step as the data sequence is traversed.\n",
    "                              If 1, the first sub-sequence will be indices 0-input_len, \n",
    "                              and the next will be 1-input_len.\n",
    "              forecast_horizon (int): How many index positions is the target away from\n",
    "                                      the last index position of the input sequence?\n",
    "                                      If forecast_horizon=1, and the input sequence\n",
    "                                      is data[0:10], the target will be data[11:taget_len].\n",
    "              target_len (int): Length of the target / output sequence.\n",
    "        \"\"\"\n",
    "        start_position = 0\n",
    "        stop_position = num_obs-1 # because of 0 indexing\n",
    "          \n",
    "        subseq_first_idx = start_position\n",
    "        subseq_last_idx = start_position + input_len\n",
    "        target_first_idx = subseq_last_idx + forecast_horizon\n",
    "        target_last_idx = target_first_idx + target_len \n",
    "        # print(\"target_last_idx is {}\".format(target_last_idx))\n",
    "        # print(\"stop_position is {}\".format(stop_position))\n",
    "        indices = []\n",
    "        while target_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_last_idx, target_first_idx, target_last_idx))\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_last_idx += step_size\n",
    "            target_first_idx = subseq_last_idx + forecast_horizon\n",
    "            target_last_idx = target_first_idx + target_len\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def get_indices_entire_sequence(data: pd.DataFrame, window_size: int, step_size: int) -> list:\n",
    "        \"\"\"\n",
    "          Produce all the start and end index positions that is needed to produce\n",
    "          the sub-sequences. \n",
    "          Returns a list of tuples. Each tuple is (start_idx, end_idx) of a sub-\n",
    "          sequence. These tuples should be used to slice the dataset into sub-\n",
    "          sequences. These sub-sequences should then be passed into a function\n",
    "          that slices them into input and target sequences. \n",
    "          \n",
    "          Args:\n",
    "              num_obs (int): Number of observations (time steps) in the entire \n",
    "                            dataset for which indices must be generated, e.g. \n",
    "                            len(data)\n",
    "              window_size (int): The desired length of each sub-sequence. Should be\n",
    "                                (input_sequence_length + target_sequence_length)\n",
    "                                E.g. if you want the model to consider the past 100\n",
    "                                time steps in order to predict the future 50 \n",
    "                                time steps, window_size = 100+50 = 150\n",
    "              step_size (int): Size of each step as the data sequence is traversed \n",
    "                              by the moving window.\n",
    "                              If 1, the first sub-sequence will be [0:window_size], \n",
    "                              and the next will be [1:window_size].\n",
    "          Return:\n",
    "              indices: a list of tuples\n",
    "        \"\"\"\n",
    "\n",
    "        stop_position = len(data)-1 # 1- because of 0 indexing\n",
    "          \n",
    "        # Start the first sub-sequence at index position 0\n",
    "        subseq_first_idx = 0\n",
    "          \n",
    "        subseq_last_idx = window_size\n",
    "          \n",
    "        indices = []\n",
    "          \n",
    "        while subseq_last_idx <= stop_position:\n",
    "\n",
    "            indices.append((subseq_first_idx, subseq_last_idx))\n",
    "              \n",
    "            subseq_first_idx += step_size\n",
    "              \n",
    "            subseq_last_idx += step_size\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def to_numeric_and_downcast_data(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Downcast columns in df to smallest possible version of it's existing data\n",
    "        type\n",
    "        \"\"\"\n",
    "        fcols = df.select_dtypes('float').columns\n",
    "    \n",
    "        icols = df.select_dtypes('integer').columns\n",
    "\n",
    "        df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    \n",
    "        df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1662555324313,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "SMGmz314LXus"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for transformer models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        indices: list, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: tensor, the entire train, validation or test data sequence \n",
    "                        before any slicing. If univariate, data.size() will be \n",
    "                        [number of samples, number of variables]\n",
    "                        where the number of variables will be equal to 1 + the number of\n",
    "                        exogenous variables. Number of exogenous variables would be 0\n",
    "                        if univariate.\n",
    "            indices: a list of tuples. Each tuple has two elements:\n",
    "                     1) the start index of a sub-sequence\n",
    "                     2) the end index of a sub-sequence. \n",
    "                     The sub-sequence is split into src, trg and trg_y later.  \n",
    "            enc_seq_len: int, the desired length of the input sequence given to the\n",
    "                     the first layer of the transformer model.\n",
    "            target_seq_len: int, the desired length of the target sequence (the output of the model)\n",
    "            target_idx: The index position of the target variable in data. Data\n",
    "                        is a 2D tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        # print(\"From get_src_trg: data size = {}\".format(data.size()))\n",
    "\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        self.target_seq_len = target_seq_len\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        # Get the first element of the i'th tuple in the list self.indicesasdfas\n",
    "        start_idx = self.indices[index][0]\n",
    "\n",
    "        # Get the second (and last) element of the i'th tuple in the list self.indices\n",
    "        end_idx = self.indices[index][1]\n",
    "\n",
    "        sequence = self.data[start_idx:end_idx]\n",
    "\n",
    "        #print(\"From __getitem__: sequence length = {}\".format(len(sequence)))\n",
    "\n",
    "        src, trg, trg_y = self.get_src_trg(\n",
    "            sequence=sequence,\n",
    "            enc_seq_len=self.enc_seq_len,\n",
    "            dec_seq_len=self.dec_seq_len,\n",
    "            target_seq_len=self.target_seq_len\n",
    "            )\n",
    "\n",
    "        return src, trg, trg_y\n",
    "    \n",
    "    def get_src_trg(\n",
    "        self,\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12971,
     "status": "ok",
     "timestamp": 1662555622384,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "cuuVCTWbPRZs",
    "outputId": "18066863-cef3-48ec-9678-4901ac7ae840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603']\n",
      "input_size is: 25\n",
      "dim_val is: 6\n"
     ]
    }
   ],
   "source": [
    "# import dataset as ds\n",
    "# import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import datetime\n",
    "# import transformer_timeseries as tst\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "data = pd.read_hdf(\"normal.h5\")\n",
    "data.drop('isAttack', inplace=True, axis=1)\n",
    "# data= data[0:4980]\n",
    "\n",
    "def norm(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "      if max(data.iloc[:,i])!= min(data.iloc[:,i]):\n",
    "        data.iloc[:,i] = norm(data.iloc[:,i])\n",
    "\n",
    "\n",
    "list_cols = list(data.columns)\n",
    "\n",
    "# Hyperparams\n",
    "batch_size = 128\n",
    "target_col_name = list_cols\n",
    "\n",
    "\n",
    "## Params\n",
    "dim_val = 6\n",
    "n_heads = 2\n",
    "n_decoder_layers = 1\n",
    "n_encoder_layers = 1\n",
    "dec_seq_len = 8 # length of input given to decoder\n",
    "enc_seq_len = 25 # length of input given to encoder\n",
    "output_sequence_length = 8 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 5 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 1024\n",
    "in_features_decoder_linear_layer = 1024\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = False\n",
    "\n",
    "# Define input variables \n",
    "exogenous_vars = [] # should contain strings. Each string must correspond to a column name\n",
    "input_variables = target_col_name \n",
    "print(input_variables)\n",
    "target_idx = 0 # index position of target in batched trg_y\n",
    "\n",
    "\n",
    "# # Remove test data from dataset\n",
    "# training_data = data[:-(round(len(data)*test_size))]\n",
    "\n",
    "TRAIN_SPLIT=0.7\n",
    "VAL_SPLIT=0.5\n",
    "\n",
    "training_data, val_data = train_test_split(data, train_size=TRAIN_SPLIT, shuffle=False, random_state=123)\n",
    "val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=False, random_state=123)\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chunkc. \n",
    "# Should be training data indices only\n",
    "training_indices = utils.get_indices_entire_sequence(\n",
    "    data=training_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "# Making instance of custom dataset class\n",
    "training_data = TransformerDataset(\n",
    "    data=torch.tensor(training_data[input_variables].values).float(),\n",
    "    indices=training_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "val_indices = utils.get_indices_entire_sequence(\n",
    "    data=val_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "val_data = TransformerDataset(\n",
    "    data=torch.tensor(val_data[input_variables].values).float(),\n",
    "    indices=val_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "test_indices = utils.get_indices_entire_sequence(\n",
    "    data=test_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "test_data = TransformerDataset(\n",
    "    data=torch.tensor(test_data[input_variables].values).float(),\n",
    "    indices=test_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# Making dataloader\n",
    "training_data = DataLoader(training_data, batch_size)\n",
    "val_data = DataLoader(val_data, batch_size)\n",
    "test_data = DataLoader(test_data, batch_size)\n",
    "\n",
    "transformer_model = TimeSeriesTransformer(\n",
    "    input_size=25,\n",
    "    dec_seq_len=enc_seq_len,\n",
    "    batch_first=batch_first,\n",
    "    num_predicted_features= output_sequence_length \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496800, 51)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1662555638764,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "3dqxs5ziBDbG"
   },
   "outputs": [],
   "source": [
    "PATH = ('transformer_model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662555641409,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "kIhMJVsZsD51"
   },
   "outputs": [],
   "source": [
    "# # optimizer = torch.optim.RMSprop(transformer_model.parameters(), \n",
    "# #                                 lr=0.0005)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.000025, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662555642325,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "M6zgvviOBQAY",
    "outputId": "c4ccfd68-b879-4512-8fcd-40e27e9ecd91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (encoder_input_layer): Linear(in_features=25, out_features=6, bias=True)\n",
       "  (decoder_input_layer): Linear(in_features=8, out_features=6, bias=True)\n",
       "  (linear_mapping): Linear(in_features=6, out_features=8, bias=True)\n",
       "  (positional_encoding_layer): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7544964,
     "status": "ok",
     "timestamp": 1662563194492,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "vnplGXkZqMry",
    "outputId": "eec1df3e-7b29-48a5-cd8b-c0f6fcbf9532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:22<00:00,  8.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "transformer_model.to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "niter = len(training_data)\n",
    "losses, val_losses = [], []\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "\n",
    "    transformer_model.train()\n",
    "    sum_train_loss = 0.0\n",
    "    \n",
    "    \n",
    "    for src, tgt, tgt_y in training_data:\n",
    "        if batch_first == False:\n",
    "            src = src.permute(2, 0, 1).to(device)\n",
    "            tgt = tgt.permute(2, 0, 1).to(device)\n",
    "            tgt_y = tgt_y.permute(2, 0, 1).to(device)\n",
    "            \n",
    "        src_mask = utils.generate_square_subsequent_mask(\n",
    "              dim1 = tgt.shape[0],\n",
    "              dim2 = src.shape[0]\n",
    "            ).to(device)\n",
    "        tgt_mask = utils.generate_square_subsequent_mask( \n",
    "              dim1= tgt.shape[0],\n",
    "              dim2= tgt.shape[0]\n",
    "            ).to(device)\n",
    "\n",
    "        temporal_embedding, output = transformer_model(\n",
    "          src=src,\n",
    "          tgt=tgt,\n",
    "          src_mask=src_mask,\n",
    "          tgt_mask=tgt_mask\n",
    "          )\n",
    "        loss = torch.nn.MSELoss()(output, tgt_y)\n",
    "        sum_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(sum_train_loss / niter)\n",
    "\n",
    "\n",
    "\n",
    "    transformer_model.eval()\n",
    "    sum_val_loss = 0.0\n",
    "\n",
    "    for i, (src, tgt, tgt_y) in enumerate(val_data):\n",
    "        if batch_first == False:\n",
    "            src = src.permute(2, 0, 1).to(device) \n",
    "            tgt = tgt.permute(2, 0, 1).to(device)\n",
    "            tgt_y = tgt_y.permute(2, 0, 1).to(device)\n",
    "\n",
    "        src_mask = utils.generate_square_subsequent_mask(\n",
    "                dim1 = tgt.shape[0],\n",
    "                dim2 = src.shape[0]\n",
    "                ).to(device)\n",
    "\n",
    "        tgt_mask = utils.generate_square_subsequent_mask( \n",
    "                  dim1= tgt.shape[0],\n",
    "                  dim2= tgt.shape[0]\n",
    "                ).to(device)\n",
    "\n",
    "        temporal_embedding, output = transformer_model(\n",
    "          src=src,\n",
    "          tgt=tgt,\n",
    "          src_mask=src_mask,\n",
    "          tgt_mask=tgt_mask\n",
    "          )\n",
    "        val_loss = torch.nn.MSELoss()(output, tgt_y)\n",
    "#         print(val_loss)\n",
    "        sum_val_loss += val_loss.item()\n",
    "    val_losses.append(sum_val_loss / (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1662563322568,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "xOfBmk0WMy9J",
    "outputId": "9881904d-8e7d-41c8-d6b8-3dff3412e1ee"
   },
   "outputs": [],
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662563326546,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "V9KNKb9d-EZR",
    "outputId": "97989a02-56dd-4cda-a0c7-c7e0e9127864"
   },
   "outputs": [],
   "source": [
    "# sum_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1299,
     "status": "ok",
     "timestamp": 1662563330845,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "2_Cbgnkc5S6t",
    "outputId": "42e5c667-3e6e-493c-e100-b36001599646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19a57689a50>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1662563334052,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "9SR-iJCC-Q_7",
    "outputId": "06edc413-5b32-4ed7-87db-cfc55f073903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19a59934d00>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1662563342578,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "grVAIttc-atl"
   },
   "outputs": [],
   "source": [
    "torch.save(transformer_model.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1662487481157,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": 240
    },
    "id": "pYvbRKB9Y5ZI",
    "outputId": "ca028822-daef-4e63-c488-3a97790bf588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15495,
     "status": "ok",
     "timestamp": 1662563400437,
     "user": {
      "displayName": "Ehtesamul Azim",
      "userId": "13532839063689012326"
     },
     "user_tz": -360
    },
    "id": "wsCCdYWAZE2p",
    "outputId": "5362020a-baea-4a20-b008-1af7de5be520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MAE loss over the test dataset is=0.1958842584465304\n",
      "mean MSE loss over the test dataset is=0.08892400932108235\n"
     ]
    }
   ],
   "source": [
    "# def mean_absolute_percentage_error(y_true, y_pred): \n",
    "#       y_true, y_pred = torch.tensor(y_true), torch.tensor(y_pred)\n",
    "#       return torch.mean(torch.abs((y_true - y_pred)/y_true)*100)\n",
    "                      \n",
    "test_losses_mape, test_losses_mae, test_losses_mse, test_preds  = [], [], [], []\n",
    "transformer_model.eval()\n",
    "\n",
    "for src, tgt, tgt_y in test_data:\n",
    "        if batch_first == False:\n",
    "            src = src.permute(2,0,1).to(device)\n",
    "            tgt = tgt.permute(2, 0, 1).to(device)\n",
    "            tgt_y = tgt_y.permute(2, 0, 1).to(device)\n",
    "\n",
    "        src_mask = utils.generate_square_subsequent_mask(\n",
    "                dim1 = tgt.shape[0],\n",
    "                dim2 = src.shape[0]\n",
    "                ).to(device)\n",
    "\n",
    "        tgt_mask = utils.generate_square_subsequent_mask( \n",
    "                  dim1= tgt.shape[0],\n",
    "                  dim2= tgt.shape[0]\n",
    "                ).to(device)\n",
    "\n",
    "        temporal_embedding, output = transformer_model(\n",
    "          src=src,\n",
    "          tgt=tgt,\n",
    "          src_mask=src_mask,\n",
    "          tgt_mask=tgt_mask\n",
    "          )\n",
    "      \n",
    "      \n",
    "        loss_test_mae = torch.nn.L1Loss()(output, tgt_y)\n",
    "        test_losses_mae.append(loss_test_mae.item())\n",
    "\n",
    "        loss_test_mse = torch.nn.MSELoss()(output, tgt_y)\n",
    "        test_losses_mse.append(loss_test_mse.item())\n",
    "#         output_test = output.permute(1,0,2)\n",
    "#         test_preds.append(output.detach().cpu().numpy())\n",
    "# test_preds = np.vstack(test_preds)\n",
    "\n",
    "print(\"mean MAE loss over the test dataset is={}\". format(np.mean(test_losses_mae)))\n",
    "print(\"mean MSE loss over the test dataset is={}\". format(np.mean(test_losses_mse)))\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size is: 25\n",
      "dim_val is: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeSeriesTransformer(\n",
    "    input_size=25,\n",
    "    dec_seq_len=enc_seq_len,\n",
    "    batch_first=batch_first,\n",
    "    num_predicted_features= output_sequence_length \n",
    "    ).to(device)\n",
    "\n",
    "PATH = 'transformer_model_2.pth'\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_embedding(data, model, batch_first=False):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        temporal_embedding=[]\n",
    "        for src, tgt, tgt_y in data:\n",
    "            if batch_first == False:\n",
    "                src = src.permute(2,0,1).to(device)\n",
    "\n",
    "            src = model.encoder_input_layer(src) \n",
    "            src = model.positional_encoding_layer(src) \n",
    "\n",
    "            temp_embedding = model.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "                            src=src\n",
    "                            )\n",
    "        \n",
    "            temp_embedding = temp_embedding.cpu()\n",
    "            \n",
    "            temporal_embedding.append(temp_embedding)\n",
    "        \n",
    "    return temporal_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_temp_embeds(X):\n",
    "    temp_embed = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        item = X[i]\n",
    "        item = torch.squeeze(item)\n",
    "        item.to('cpu')\n",
    "        item = item.numpy()\n",
    "        temp_embed.append(item)\n",
    "        \n",
    "    temp_embed = np.asarray(temp_embed)\n",
    "    return temp_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_dataset\n",
    "\n",
    "normal_data = pd.read_hdf(\"normal.h5\")\n",
    "normal_data.drop('isAttack', inplace=True, axis=1)\n",
    "# normal_data = normal_data[0:4980]\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "\n",
    "for i in range(normal_data.shape[1]):\n",
    "      if max(normal_data.iloc[:,i])!= min(normal_data.iloc[:,i]):\n",
    "        normal_data.iloc[:,i] = norm(normal_data.iloc[:,i])\n",
    "\n",
    "normal_indices = utils.get_indices_entire_sequence(\n",
    "    data=data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "normal_data = TransformerDataset(\n",
    "    data=torch.tensor(data[input_variables].values).float(),\n",
    "    indices=normal_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# # Making dataloader\n",
    "normal_data = DataLoader(normal_data, batch_size=1)\n",
    "\n",
    "\n",
    "# attack_dataset\n",
    "\n",
    "attack_data = pd.read_hdf('attack.h5')\n",
    "attack_data.drop('isAttack', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "for i in range(attack_data.shape[1]):\n",
    "      if max(attack_data.iloc[:,i])!= min(attack_data.iloc[:,i]):\n",
    "        attack_data.iloc[:,i] = norm(attack_data.iloc[:,i])\n",
    "list_sensors = list(attack_data.columns)\n",
    "\n",
    "attack_indices = utils.get_indices_entire_sequence(\n",
    "    data=attack_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "attack_data = TransformerDataset(\n",
    "    data=torch.tensor(attack_data[list_sensors].values).float(),\n",
    "    indices=attack_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# Making dataloader\n",
    "attack_data = DataLoader(attack_data, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src, tgt, tgt_y in attack_data:\n",
    "#     src = src.numpy()\n",
    "#     print(src.shape)\n",
    "\n",
    "# np.savez_compressed(\"attack_matrix\", src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src, tgt, tgt_y in normal_data:\n",
    "#     src = src.numpy()\n",
    "    \n",
    "# np.savez_compressed(\"normal_matrix\", src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89978"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = next(iter(normal_data))\n",
    "x.shape\n",
    "len(attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_20860\\3860963174.py:2: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  normal_temporal_embedding = np.asarray(normal_temporal_embedding)\n",
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_20860\\3860963174.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  normal_temporal_embedding = np.asarray(normal_temporal_embedding)\n"
     ]
    }
   ],
   "source": [
    "normal_temporal_embedding = get_temporal_embedding(data=normal_data, model=model)\n",
    "normal_temporal_embedding = np.asarray(normal_temporal_embedding)\n",
    "normal_temp_embed = mod_temp_embeds(normal_temporal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99354, 51, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_temp_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"normal_temporal_embedding.npz\",normal_temp_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_20860\\2993424679.py:2: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  attack_temporal_embedding = np.asarray(attack_temporal_embedding)\n",
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_20860\\2993424679.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  attack_temporal_embedding = np.asarray(attack_temporal_embedding)\n"
     ]
    }
   ],
   "source": [
    "attack_temporal_embedding = get_temporal_embedding(data=attack_data, model=model)\n",
    "attack_temporal_embedding = np.asarray(attack_temporal_embedding)\n",
    "attack_temp_embed = mod_temp_embeds(attack_temporal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"attack_temporal_embedding.npz\",attack_temp_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19869, 51, 8)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = np.load(\"normal_temporal_embedding.npz\")[\"arr_0\"]\n",
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29987, 51, 16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack = np.load(\"attack_temporal_embedding.npz\")[\"arr_0\"]\n",
    "attack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89978,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = np.load(\"newlabel_ehtesam.npz\")[\"arr_0\"]\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4579425 ,  1.0832533 , -0.74690086,  1.2021334 , -0.76193094,\n",
       "       -0.7691085 , -1.0191563 , -0.9217193 ], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack[100,43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4579663 ,  1.0832623 , -0.7468552 ,  1.2020949 , -0.76188153,\n",
       "       -0.76914454, -1.0191631 , -0.921756  ], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack[2, 43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7481824 ,  0.8719848 , -0.7792775 , -0.50017756, -0.7648723 ,\n",
       "       -0.9156781 ,  0.71447754,  0.565138  ], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal[125,43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7481824 ,  0.8719848 , -0.7792775 , -0.50017756, -0.7648723 ,\n",
       "       -0.9156781 ,  0.71447754,  0.565138  ], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal[320,43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7295868 , -0.06217688,  0.09408717,  1.049709  ,  0.814017  ,\n",
       "       -0.8237257 ,  1.0709875 ,  0.29199362, -0.35954526,  0.799839  ,\n",
       "        0.8571277 ,  1.3902394 , -0.9075305 , -0.14469382, -0.74356574,\n",
       "        0.38241002], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal[900,43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4538, -0.2701, -0.4398, -0.1409, -0.4429, -0.3758, -0.2304, -0.1857,\n",
      "         -0.3239, -0.2338, -0.3373, -0.2027, -0.3353, -0.4373, -0.2311, -0.3610,\n",
      "         -0.2635, -0.4163, -0.1515, -0.4030, -0.3558, -0.3782, -0.1805, -0.2047,\n",
      "         -0.1759],\n",
      "        [ 0.3961,  0.4494,  0.1492,  0.2543,  0.2449,  0.2459,  0.1655,  0.5146,\n",
      "          0.2694,  0.3462,  0.4701,  0.4781,  0.1442,  0.3162,  0.2654,  0.4356,\n",
      "          0.4596,  0.1905,  0.2220,  0.4229,  0.5270,  0.2126,  0.1606,  0.3462,\n",
      "          0.5162],\n",
      "        [ 0.2327,  0.3325,  0.5104,  0.4169,  0.3744,  0.2273,  0.5243,  0.3295,\n",
      "          0.3064,  0.3645,  0.3059,  0.2649,  0.2353,  0.3190,  0.4636,  0.5157,\n",
      "          0.1994,  0.3281,  0.2890,  0.1623,  0.2992,  0.4834,  0.3727,  0.4598,\n",
      "          0.3554],\n",
      "        [ 0.1950,  0.1918,  0.5399,  0.4330,  0.2199,  0.4007,  0.4329,  0.3163,\n",
      "          0.3072,  0.2565,  0.3705,  0.3214,  0.3836,  0.3533,  0.5200,  0.3922,\n",
      "          0.3015,  0.4710,  0.3370,  0.1424,  0.2422,  0.2568,  0.4085,  0.2656,\n",
      "          0.4369],\n",
      "        [ 0.2221,  0.2542,  0.3705,  0.3686,  0.3077,  0.3514,  0.3763,  0.1875,\n",
      "          0.4037,  0.3771,  0.3946,  0.2059,  0.3816,  0.1574,  0.4910,  0.3093,\n",
      "          0.3531,  0.4995,  0.2056,  0.4676,  0.4634,  0.2809,  0.3918,  0.2395,\n",
      "          0.2539],\n",
      "        [-0.5229, -0.1495, -0.2271, -0.5074, -0.2915, -0.2182, -0.2942, -0.4477,\n",
      "         -0.4756, -0.4942, -0.3764, -0.1426, -0.1783, -0.1635, -0.4383, -0.4825,\n",
      "         -0.3313, -0.4096, -0.1900, -0.3176, -0.4026, -0.2253, -0.2489, -0.4469,\n",
      "         -0.1484],\n",
      "        [ 0.3793,  0.5398,  0.2162,  0.2604,  0.2622,  0.2967,  0.2735,  0.1547,\n",
      "          0.4364,  0.1405,  0.4957,  0.3791,  0.2268,  0.3916,  0.3847,  0.4046,\n",
      "          0.4435,  0.3887,  0.2626,  0.1950,  0.4286,  0.5093,  0.3497,  0.4788,\n",
      "          0.3963],\n",
      "        [-0.3730, -0.4824, -0.2982, -0.3046, -0.3721, -0.3873, -0.4612, -0.2946,\n",
      "         -0.2689, -0.4006, -0.4420, -0.2185, -0.3761, -0.1627, -0.2301, -0.3684,\n",
      "         -0.4450, -0.3012, -0.2067, -0.4391, -0.2711, -0.3929, -0.4761, -0.4846,\n",
      "         -0.5173],\n",
      "        [-0.2013, -0.3358, -0.4754, -0.4579, -0.4702, -0.3156, -0.4293, -0.3887,\n",
      "         -0.3860, -0.1868, -0.2560, -0.4798, -0.1419, -0.3881, -0.1431, -0.4266,\n",
      "         -0.4208, -0.4763, -0.3263, -0.1526, -0.3021, -0.2871, -0.4005, -0.4596,\n",
      "         -0.3030],\n",
      "        [ 0.4197,  0.2648,  0.3862,  0.3870,  0.2094,  0.2504,  0.1792,  0.2482,\n",
      "          0.2438,  0.4198,  0.2518,  0.3030,  0.3900,  0.2161,  0.2731,  0.1502,\n",
      "          0.1941,  0.1341,  0.3698,  0.2157,  0.1782,  0.3015,  0.2663,  0.2841,\n",
      "          0.3512],\n",
      "        [ 0.3605,  0.4988,  0.4816,  0.4853,  0.2814,  0.3197,  0.1740,  0.5289,\n",
      "          0.3680,  0.3332,  0.3487,  0.2002,  0.5329,  0.5279,  0.3624,  0.5248,\n",
      "          0.4379,  0.2271,  0.2892,  0.4117,  0.3935,  0.4009,  0.2917,  0.3983,\n",
      "          0.3662],\n",
      "        [ 0.5325,  0.4978,  0.1613,  0.4541,  0.2888,  0.4716,  0.2306,  0.2014,\n",
      "          0.5296,  0.3031,  0.3507,  0.1822,  0.4310,  0.3887,  0.1861,  0.5378,\n",
      "          0.3600,  0.2066,  0.3114,  0.2227,  0.5051,  0.3181,  0.2719,  0.2093,\n",
      "          0.2219],\n",
      "        [-0.2253, -0.3857, -0.3915, -0.1575, -0.3586, -0.2852, -0.2362, -0.1732,\n",
      "         -0.3380, -0.5355, -0.3959, -0.4725, -0.4446, -0.2581, -0.1818, -0.2000,\n",
      "         -0.5081, -0.5136, -0.2970, -0.1682, -0.3122, -0.3997, -0.5238, -0.4854,\n",
      "         -0.4221],\n",
      "        [-0.0282, -0.2281, -0.1959, -0.0228, -0.2415, -0.2925, -0.1053,  0.0172,\n",
      "         -0.3030,  0.0093, -0.0530, -0.0552, -0.1537, -0.3544, -0.1445, -0.0386,\n",
      "         -0.2180, -0.0443, -0.1690, -0.3194, -0.1547, -0.0878, -0.0820, -0.2338,\n",
      "         -0.0847],\n",
      "        [-0.2583, -0.4336, -0.3132, -0.2668, -0.3080, -0.3248, -0.3327, -0.1781,\n",
      "         -0.4066, -0.2396, -0.1617, -0.1726, -0.2439, -0.2234, -0.3885, -0.3101,\n",
      "         -0.3056, -0.5327, -0.3822, -0.3451, -0.4667, -0.3918, -0.2013, -0.5024,\n",
      "         -0.3143],\n",
      "        [-0.4800, -0.2534, -0.4605, -0.2263, -0.2180, -0.1750, -0.4854, -0.1704,\n",
      "         -0.3070, -0.3834, -0.4224, -0.1799, -0.5114, -0.1743, -0.2734, -0.2982,\n",
      "         -0.4867, -0.3365, -0.4676, -0.5042, -0.1859, -0.2293, -0.2821, -0.4740,\n",
      "         -0.4826]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3601,  0.3417,  0.5308,  0.4872,  0.2377, -0.3986,  0.2555, -0.2111,\n",
      "        -0.2609,  0.4436,  0.4707,  0.4199, -0.2825, -0.4249, -0.2564, -0.2777],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0055,  0.4580,  0.2055,  0.6826,  0.0710,  0.1878,  0.2455,  0.0325],\n",
      "        [-0.4991, -0.2485, -0.6188, -0.5341, -0.3603, -0.3135, -0.1017, -0.1300],\n",
      "        [-0.1513, -0.1595, -0.6036, -0.1600, -0.5361, -0.0480, -0.4428, -0.6282],\n",
      "        [ 0.4395,  0.2346,  0.3660,  0.0059,  0.1171,  0.6506,  0.5183,  0.2780],\n",
      "        [ 0.3433, -0.0844, -0.1938, -0.1201,  0.1245,  0.0693, -0.0581,  0.1247],\n",
      "        [ 0.0059,  0.4077,  0.5376,  0.0696, -0.2183, -0.1872, -0.1732, -0.1537],\n",
      "        [-0.5137, -0.3095, -0.0557,  0.0116, -0.3994, -0.3045, -0.0364, -0.2888],\n",
      "        [ 0.0118,  0.5353,  0.6341,  0.5125,  0.5426,  0.1294,  0.0797,  0.4473],\n",
      "        [ 0.5991,  0.3965,  0.5113,  0.1549,  0.3589,  0.2654,  0.5217,  0.4303],\n",
      "        [-0.3850,  0.2645, -0.0707, -0.1279, -0.0630,  0.1465, -0.2723, -0.0073],\n",
      "        [ 0.3089,  0.5294,  0.5469,  0.3762,  0.3699,  0.1866,  0.5179,  0.1907],\n",
      "        [-0.3265, -0.4243, -0.1214, -0.4042, -0.1744, -0.6010, -0.5130, -0.5393],\n",
      "        [ 0.4781,  0.2893,  0.2183,  0.3598,  0.3139,  0.1592,  0.6740,  0.0460],\n",
      "        [ 0.4645,  0.1556,  0.5226,  0.2008,  0.2332,  0.1579,  0.3257,  0.2880],\n",
      "        [ 0.5261,  0.4553,  0.6264,  0.3048,  0.2643,  0.6103,  0.1935,  0.6744],\n",
      "        [ 0.2185,  0.4345, -0.0049,  0.0747,  0.1711,  0.5157,  0.0702,  0.4426]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4025,  0.5102,  0.1023, -0.6675,  0.1434,  0.5113, -0.4510, -0.5396,\n",
      "         0.3294, -0.1424,  0.5381,  0.0434,  0.5276,  0.3110, -0.2173,  0.5616],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.4891e-02, -2.1427e-01, -4.6107e-01,  4.6840e-02, -3.7783e-01,\n",
      "         -1.5310e-01, -2.8633e-01,  2.5204e-01,  5.3301e-01, -7.5820e-02,\n",
      "          2.2154e-01, -2.7105e-01,  2.7674e-01,  3.1024e-02,  4.9463e-01,\n",
      "          5.6375e-01],\n",
      "        [-1.4395e-01, -4.9186e-01, -4.9158e-01, -1.6427e-01, -5.2194e-01,\n",
      "         -1.8636e-01, -5.1455e-01,  5.2236e-01,  1.0478e-01, -1.8237e-01,\n",
      "         -6.0172e-02, -5.7538e-01,  2.5342e-01, -6.3022e-02,  3.6365e-01,\n",
      "          5.0010e-01],\n",
      "        [-1.4999e-01, -2.8216e-01, -3.7604e-01, -6.3333e-02, -3.5126e-01,\n",
      "         -4.9931e-01, -1.9737e-01,  3.7035e-01,  2.8507e-01, -1.5568e-01,\n",
      "          9.2463e-02, -4.7721e-01,  5.6951e-01,  1.9962e-02,  2.8220e-01,\n",
      "          5.4938e-01],\n",
      "        [-1.4712e-01, -4.1305e-01, -4.0873e-01, -9.3667e-02, -2.1268e-01,\n",
      "         -4.7192e-01, -3.8698e-01,  1.0255e-01,  2.4334e-01, -1.2128e-01,\n",
      "         -1.0549e-01, -5.1620e-01,  4.3346e-01, -2.8939e-02,  3.6882e-01,\n",
      "          2.5551e-01],\n",
      "        [-1.2207e-02, -3.2773e-01, -1.3929e-01,  6.0225e-02, -2.5088e-01,\n",
      "         -5.4082e-01, -3.9070e-01,  4.7273e-01,  4.7939e-01,  5.4871e-02,\n",
      "          3.3244e-01, -1.6200e-01,  2.0376e-01, -1.5432e-01,  2.7810e-01,\n",
      "          1.9192e-01],\n",
      "        [ 3.5189e-02, -2.4273e-01, -2.7840e-01, -9.1544e-02, -4.7990e-01,\n",
      "         -2.7474e-01, -3.6167e-01,  2.3048e-01,  3.9292e-01,  5.1237e-03,\n",
      "          7.1842e-02, -5.8149e-01,  1.9912e-01, -4.6779e-02,  5.2503e-01,\n",
      "          1.7093e-01],\n",
      "        [ 4.1571e-02,  5.1064e-03, -9.3255e-02,  1.9003e-02, -4.8424e-01,\n",
      "         -2.9499e-01, -3.6496e-01,  3.8112e-01,  3.1448e-01, -2.0638e-01,\n",
      "         -1.1012e-01, -5.7007e-01,  4.4620e-01, -1.5631e-01,  2.8908e-01,\n",
      "          4.1117e-01],\n",
      "        [-1.6969e-01, -1.8477e-01, -5.6510e-01,  1.6335e-02, -5.3475e-01,\n",
      "         -3.7553e-01, -1.4610e-01,  2.0475e-01,  3.2164e-01, -5.2726e-02,\n",
      "         -1.1963e-01, -4.6237e-01,  4.1717e-01,  1.6188e-04,  4.0125e-01,\n",
      "          9.1749e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2603, 0.1958, 0.3043, 0.4650, 0.1984, 0.3615, 0.3021, 0.5630],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4646, -0.3282, -0.4327, -0.2439, -0.2560, -0.0692, -0.5799, -0.1159,\n",
      "          0.4851,  0.3413, -0.3337, -0.0857,  0.1630, -0.3438, -0.1901, -0.2206],\n",
      "        [-0.3756,  0.3871, -0.2875, -0.2893, -0.1603, -0.4342, -0.2023, -0.6145,\n",
      "         -0.5436, -0.0736, -0.0489, -0.3348,  0.2560, -0.0751, -0.1110, -0.5969],\n",
      "        [-0.2925,  0.0911, -0.0856, -0.4420, -0.5444, -0.3304, -0.4248, -0.0465,\n",
      "         -0.6363,  0.1482, -0.5346, -0.5502, -0.0981, -0.3332, -0.5929, -0.1548],\n",
      "        [ 0.0935, -0.0813,  0.1484,  0.0650,  0.4480,  0.3543,  0.4404,  0.3588,\n",
      "          0.1568,  0.0648,  0.2050,  0.5417, -0.0336,  0.3838,  0.5425,  0.4128],\n",
      "        [ 0.0473,  0.1499,  0.1998,  0.0827, -0.4862,  0.5098, -0.4171, -0.5768,\n",
      "         -0.4597,  0.1380, -0.2121, -0.0464, -0.3218, -0.0671, -0.2240, -0.3379],\n",
      "        [-0.5995, -0.3541, -0.5472, -0.5550, -0.4338,  0.0145, -0.5323,  0.2888,\n",
      "         -0.0945,  0.1478, -0.4230, -0.5329,  0.0379, -0.3006,  0.1534, -0.3909],\n",
      "        [ 0.3143, -0.5474,  0.5066,  0.4403, -0.0768,  0.3348, -0.0660,  0.6406,\n",
      "         -0.4493,  0.4754,  0.4076,  0.1225, -0.4341,  0.4816,  0.4238,  0.6104],\n",
      "        [ 0.0715, -0.6259,  0.0495,  0.2390, -0.2362,  0.6136,  0.3799,  0.0691,\n",
      "          0.5350,  0.0214,  0.3418,  0.4711,  0.1812,  0.2556,  0.2845,  0.4953],\n",
      "        [-0.5707,  0.5734, -0.3077, -0.3987,  0.0214, -0.3041, -0.4906, -0.3799,\n",
      "         -0.5260,  0.1109, -0.2219, -0.5306,  0.2713, -0.0447, -0.2051, -0.2116],\n",
      "        [-0.5911,  0.1445, -0.4786,  0.0968, -0.0686, -0.3117, -0.1064, -0.4669,\n",
      "         -0.0720,  0.0633, -0.4121, -0.0921,  0.1234, -0.4638, -0.4787, -0.5034],\n",
      "        [-0.2436,  0.1331,  0.0270,  0.0606,  0.2906, -0.5530,  0.1936, -0.3511,\n",
      "         -0.2372, -0.1767, -0.1589, -0.1266, -0.3479, -0.3660, -0.5161, -0.2672],\n",
      "        [-0.1857,  0.0132, -0.4819, -0.1875, -0.0820, -0.1490, -0.2137,  0.3499,\n",
      "          0.5596, -0.1035, -0.2795, -0.1725,  0.4037, -0.1378,  0.3933, -0.0467],\n",
      "        [-0.6080,  0.2133, -0.0923, -0.0201,  0.4295, -0.1650,  0.3338, -0.3468,\n",
      "         -0.2142, -0.0049, -0.2792, -0.1031, -0.1884, -0.1710, -0.1945, -0.5566],\n",
      "        [ 0.1862, -0.0880, -0.2646, -0.3674, -0.2425,  0.1002,  0.0188,  0.1926,\n",
      "          0.1396, -0.4180, -0.1438,  0.0502,  0.6319,  0.3069,  0.1200,  0.3728],\n",
      "        [-0.1004,  0.1590, -0.4134, -0.1356, -0.0727, -0.3085, -0.0728, -0.4159,\n",
      "          0.3253, -0.1481, -0.2262, -0.5831,  0.5820, -0.2512, -0.4081, -0.1311],\n",
      "        [-0.5943, -0.0660, -0.2574, -0.2243, -0.4202, -0.3009, -0.6221, -0.5979,\n",
      "         -0.3998, -0.2759, -0.4179, -0.2104, -0.0906, -0.4117, -0.3517, -0.5636],\n",
      "        [-0.3474,  0.0609, -0.2883, -0.6207, -0.2366, -0.4317,  0.4106, -0.6440,\n",
      "         -0.0909,  0.4618,  0.6055,  0.2664, -0.2241, -0.3646, -0.0785, -0.2347],\n",
      "        [-0.3998,  0.3043, -0.0819, -0.2796, -0.4619, -0.2954, -0.2344,  0.2358,\n",
      "          0.5601,  0.5400, -0.2835,  0.2416,  0.4293, -0.0507,  0.2360,  0.0939],\n",
      "        [ 0.4149, -0.4837,  0.1085, -0.4760,  0.2479,  0.5984,  0.2384, -0.0181,\n",
      "         -0.4637,  0.2602,  0.4583,  0.0591, -0.3628,  0.5461, -0.4308, -0.3083],\n",
      "        [ 0.1739, -0.1895,  0.3402,  0.1225, -0.4976, -0.5423,  0.3296, -0.3849,\n",
      "          0.4246, -0.0640,  0.4663,  0.1188, -0.2637,  0.1574,  0.0668, -0.2176],\n",
      "        [-0.3099,  0.3639, -0.4030, -0.1627, -0.1581, -0.2317, -0.4623, -0.4771,\n",
      "          0.5515,  0.2726,  0.5377,  0.2066,  0.5314, -0.3602, -0.5236, -0.0964],\n",
      "        [ 0.5422, -0.0956,  0.4788,  0.1249,  0.3102,  0.6409,  0.5682,  0.0537,\n",
      "         -0.2943, -0.3187, -0.5156, -0.3367, -0.2232,  0.3489,  0.2789,  0.0880],\n",
      "        [-0.6292,  0.0844, -0.5005, -0.5200, -0.5082, -0.2496, -0.1363, -0.5909,\n",
      "          0.4757,  0.4967,  0.3913,  0.3634, -0.4863, -0.2038, -0.5035, -0.1595],\n",
      "        [-0.0967,  0.4593, -0.4349, -0.2974,  0.0231, -0.5806,  0.0422, -0.1364,\n",
      "          0.0348,  0.5534,  0.4569,  0.0674, -0.5222, -0.6369, -0.3240, -0.3277],\n",
      "        [-0.4144, -0.2012,  0.4089, -0.0887,  0.1899, -0.4096, -0.5672,  0.0626,\n",
      "          0.1615,  0.3752, -0.1289,  0.3261,  0.4121, -0.2787, -0.1554,  0.4134],\n",
      "        [ 0.1996,  0.1986, -0.5948,  0.5954, -0.4039,  0.2886, -0.1511,  0.5380,\n",
      "          0.5766,  0.0221, -0.0608, -0.5687,  0.3275,  0.1293,  0.6263,  0.3298],\n",
      "        [ 0.2737, -0.1751, -0.5515, -0.3397, -0.1849,  0.3655, -0.0926,  0.5557,\n",
      "          0.2959,  0.3297, -0.0936, -0.4660,  0.5520, -0.2111,  0.4083,  0.1518],\n",
      "        [-0.2513, -0.1771,  0.2793, -0.2426,  0.0855, -0.5492,  0.0328, -0.3516,\n",
      "         -0.4012,  0.1157,  0.0214,  0.5922, -0.2215, -0.4993, -0.3918, -0.0918],\n",
      "        [ 0.0729, -0.1212, -0.2680,  0.4942, -0.5304,  0.3037, -0.5006,  0.4167,\n",
      "          0.0842,  0.0376, -0.4214, -0.5539,  0.4773,  0.2504,  0.5600,  0.5212],\n",
      "        [-0.0976, -0.2104,  0.1038, -0.1183,  0.6086, -0.5674,  0.5266, -0.3233,\n",
      "         -0.0420, -0.1044,  0.2011,  0.2556, -0.2540,  0.3182,  0.2466, -0.0591],\n",
      "        [-0.2300, -0.0659,  0.1148, -0.3835,  0.5078, -0.3632, -0.1818,  0.3009,\n",
      "         -0.0711,  0.4388, -0.0749,  0.1685,  0.0140, -0.1389, -0.2771, -0.4344],\n",
      "        [ 0.6001,  0.1473, -0.1996,  0.6019, -0.2877,  0.3933, -0.5865,  0.1486,\n",
      "          0.1092,  0.0876, -0.2378, -0.2533,  0.0056,  0.6103,  0.6340,  0.3327],\n",
      "        [-0.1539,  0.0581, -0.1262, -0.2867, -0.2111,  0.5939, -0.2063, -0.2937,\n",
      "         -0.2886, -0.0075, -0.0436, -0.1672,  0.1415, -0.4772, -0.0854, -0.1883],\n",
      "        [-0.2464,  0.1555, -0.2110, -0.2869,  0.1337,  0.2659, -0.5101, -0.6055,\n",
      "         -0.2700, -0.3370, -0.3691, -0.1630,  0.0929, -0.5601, -0.3495, -0.1695],\n",
      "        [ 0.1056, -0.1107,  0.0802,  0.1769, -0.4856,  0.1284,  0.2013,  0.5747,\n",
      "          0.1790, -0.0460,  0.4592,  0.1900,  0.0764,  0.0627,  0.5461,  0.2988],\n",
      "        [ 0.4720, -0.6113,  0.4804,  0.3442, -0.4347, -0.1433,  0.4880,  0.5517,\n",
      "          0.1917,  0.4075,  0.5797,  0.0622, -0.3751,  0.0626,  0.3947,  0.1007],\n",
      "        [-0.3421,  0.1119, -0.2883, -0.2463,  0.5627,  0.5672, -0.2060, -0.1877,\n",
      "         -0.3711, -0.3162, -0.5993, -0.6018,  0.5248, -0.1810, -0.5203, -0.1252],\n",
      "        [-0.1584,  0.5561, -0.2700, -0.1281,  0.4830, -0.1658, -0.1405, -0.1984,\n",
      "         -0.1867, -0.0118, -0.6252, -0.5588,  0.2187, -0.2245, -0.1962, -0.5279],\n",
      "        [-0.1786,  0.0570, -0.1749, -0.6198,  0.4239,  0.1173, -0.0369, -0.4506,\n",
      "         -0.4408, -0.1498, -0.4612, -0.5774,  0.2779, -0.5920, -0.4665, -0.5798],\n",
      "        [ 0.5180, -0.6042,  0.4778,  0.3833, -0.3208, -0.0459,  0.0816,  0.0776,\n",
      "          0.3573,  0.5510,  0.5589,  0.4770, -0.1676,  0.0369,  0.1868,  0.1859],\n",
      "        [ 0.2521, -0.5939, -0.1956, -0.2898, -0.3181,  0.3473, -0.1911,  0.2783,\n",
      "          0.4264,  0.5768, -0.0507, -0.1459,  0.5951, -0.0292,  0.0478,  0.1264],\n",
      "        [ 0.1107, -0.1233,  0.5046,  0.4547, -0.2862, -0.2170,  0.3387,  0.3819,\n",
      "          0.2736,  0.3119,  0.6131,  0.4055, -0.1845,  0.1910,  0.0733,  0.6398],\n",
      "        [ 0.4797, -0.3870,  0.1847,  0.3310, -0.3324, -0.0935,  0.2653,  0.3464,\n",
      "          0.2897,  0.5974,  0.3269,  0.5797,  0.0603,  0.1286,  0.2173,  0.5144],\n",
      "        [ 0.2171, -0.6023,  0.5912,  0.3736, -0.3047, -0.5555,  0.3780,  0.1638,\n",
      "          0.0632,  0.2598,  0.0750,  0.3021, -0.5205,  0.2121,  0.3843,  0.1630],\n",
      "        [ 0.4489, -0.1684,  0.4580,  0.0496, -0.0979, -0.6364,  0.2665,  0.2883,\n",
      "          0.5044,  0.2403,  0.1592,  0.3322, -0.4187,  0.3833,  0.2910,  0.2956],\n",
      "        [ 0.1457, -0.1050,  0.4725,  0.3505, -0.4337, -0.4201,  0.3752,  0.2715,\n",
      "          0.3118,  0.4247,  0.3343,  0.6390, -0.4407,  0.6384,  0.5213,  0.3971],\n",
      "        [-0.0777,  0.1570, -0.3448, -0.4711,  0.0364,  0.6257, -0.2565, -0.3623,\n",
      "         -0.3161, -0.4694, -0.3679, -0.1942,  0.3804, -0.5527, -0.0736, -0.4328],\n",
      "        [-0.5174,  0.5749, -0.3939, -0.0898,  0.1322,  0.4635, -0.1457, -0.2205,\n",
      "         -0.0575, -0.1209, -0.4933, -0.2663,  0.2886, -0.0894, -0.3479, -0.1790]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3399, -0.3403, -0.3405,  0.3407, -0.3403, -0.3376,  0.3396,  0.3402,\n",
      "        -0.3399, -0.3407, -0.3411, -0.3404, -0.3404,  0.2377, -0.3401, -0.3405,\n",
      "         0.0145,  0.0059, -0.0143,  0.0161,  0.0217, -0.0221,  0.0284, -0.0040,\n",
      "         0.0063,  0.0051,  0.0018, -0.0067, -0.0067,  0.0098,  0.0099, -0.0074,\n",
      "        -0.3408, -0.3400,  0.3406,  0.3402, -0.3405, -0.3410, -0.3404,  0.3409,\n",
      "        -0.2702,  0.3409,  0.3405,  0.3408,  0.3408,  0.3403, -0.3412, -0.3407],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4410,  0.2513,  0.4148,  0.2046, -0.2883,  0.3790,  0.4022,  0.3599,\n",
      "         -0.5829, -0.2171, -0.4748,  0.4933,  0.4854, -0.4196,  0.0524, -0.1439],\n",
      "        [ 0.4559, -0.2783, -0.4223,  0.5815, -0.4379, -0.1605, -0.2847, -0.0338,\n",
      "          0.1000,  0.0592, -0.2044,  0.0919,  0.2640,  0.4409, -0.0218, -0.3584],\n",
      "        [ 0.5153,  0.3804, -0.4219, -0.4853,  0.3361, -0.4322, -0.3976, -0.4673,\n",
      "          0.5721,  0.0845, -0.4642, -0.2642, -0.2630,  0.2189,  0.4746,  0.1493],\n",
      "        [ 0.3047, -0.4553, -0.1549, -0.4476,  0.0972, -0.4430, -0.1285, -0.2023,\n",
      "          0.4193,  0.5059,  0.2360, -0.2092, -0.4068,  0.5670,  0.0655,  0.2773],\n",
      "        [ 0.4751, -0.5466, -0.3584, -0.1483,  0.1597, -0.3739, -0.4220, -0.2116,\n",
      "          0.4339,  0.4031,  0.4829, -0.5591, -0.1692,  0.2822, -0.3934,  0.1773],\n",
      "        [-0.2514,  0.2616,  0.5873,  0.5853,  0.3004,  0.3350,  0.3349,  0.1362,\n",
      "         -0.3593, -0.3667, -0.2376,  0.1221,  0.1833, -0.4884,  0.2393, -0.1526],\n",
      "        [ 0.4041, -0.2908, -0.3390, -0.2007,  0.2466, -0.3635, -0.5388, -0.5346,\n",
      "          0.2969,  0.5752,  0.0497, -0.4044, -0.5468,  0.4978,  0.2367,  0.1473],\n",
      "        [-0.4996,  0.1672,  0.5433,  0.3702, -0.4363,  0.0931,  0.3559,  0.3009,\n",
      "         -0.1720, -0.3893, -0.3007,  0.5155,  0.1273, -0.2662, -0.2065, -0.2855],\n",
      "        [-0.1004,  0.4467,  0.1674,  0.3115,  0.3132,  0.4229,  0.3543,  0.1428,\n",
      "         -0.1213, -0.5297, -0.5580,  0.1951,  0.1353, -0.2038,  0.3972, -0.5207],\n",
      "        [ 0.2395, -0.1458, -0.1778, -0.1019, -0.3913, -0.1654, -0.1438, -0.5317,\n",
      "          0.2281,  0.4985,  0.4794, -0.1147,  0.2100,  0.1827, -0.4215,  0.2546],\n",
      "        [ 0.1851, -0.3827, -0.1881, -0.3211, -0.1672, -0.4229, -0.2409, -0.1027,\n",
      "          0.1868,  0.5022,  0.1134, -0.3980, -0.2152,  0.5139, -0.5550,  0.1972],\n",
      "        [ 0.4659, -0.3595, -0.2419, -0.4762,  0.5048, -0.4892, -0.3524, -0.2273,\n",
      "          0.4632,  0.3490, -0.3775, -0.2485, -0.4202,  0.1887,  0.2638,  0.3473],\n",
      "        [-0.2009,  0.4710,  0.1027,  0.3628, -0.2958,  0.1946,  0.4039,  0.4635,\n",
      "         -0.2212, -0.2103,  0.0416,  0.1209,  0.1944, -0.4309, -0.3981, -0.2029],\n",
      "        [ 0.2433, -0.1325, -0.2054, -0.2041,  0.4089, -0.4207, -0.4198, -0.1365,\n",
      "          0.3424,  0.5366, -0.0341, -0.2366, -0.2038,  0.1882,  0.1079,  0.5771],\n",
      "        [-0.2814,  0.3692,  0.4666,  0.3392,  0.2703,  0.3808,  0.5175,  0.1452,\n",
      "         -0.1511, -0.5892, -0.2950,  0.1240,  0.1733, -0.4946,  0.4705, -0.2767],\n",
      "        [-0.2263, -0.2742,  0.3997,  0.2567, -0.2571,  0.4381,  0.5111,  0.5159,\n",
      "         -0.4862, -0.1890,  0.1162,  0.1240,  0.4055, -0.5743, -0.4499, -0.4719]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3406,  0.2183,  0.3399,  0.3404,  0.3405, -0.3411,  0.3406, -0.3398,\n",
      "        -0.3394,  0.3411,  0.3401,  0.3403, -0.3406,  0.3404, -0.3408, -0.3388],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5526, -0.0842, -0.5462,  ...,  0.5359, -0.2175,  0.5362],\n",
      "        [ 0.1596, -0.1646,  0.5688,  ..., -0.2127,  0.3772,  0.5706],\n",
      "        [ 0.3450,  0.5222, -0.1076,  ..., -0.4782,  0.1829,  0.5553],\n",
      "        ...,\n",
      "        [ 0.1361,  0.4850, -0.4093,  ...,  0.5886,  0.1457,  0.3755],\n",
      "        [ 0.5479, -0.2541,  0.2967,  ..., -0.0939,  0.2292, -0.2341],\n",
      "        [-0.1682, -0.0508, -0.0756,  ...,  0.1158, -0.0839,  0.0290]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5185,  0.2902,  0.2383,  ...,  0.2145, -0.2393, -0.4554],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3431, -0.3716, -0.3131,  ..., -0.3382, -0.3595, -0.3241],\n",
      "        [-0.3330,  0.3242, -0.3446,  ..., -0.3083,  0.3555,  0.3630],\n",
      "        [-0.2698,  0.3648,  0.0898,  ..., -0.2979,  0.3364,  0.3229],\n",
      "        ...,\n",
      "        [-0.3617, -0.3128, -0.1537,  ..., -0.3435,  0.3579,  0.3343],\n",
      "        [-0.3608, -0.3352, -0.3284,  ..., -0.3509, -0.3155, -0.3195],\n",
      "        [-0.3472, -0.3385,  0.3471,  ...,  0.2589, -0.3503,  0.3180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3697,  0.3095,  0.3253,  0.3680,  0.3673, -0.3436,  0.3615,  0.3488,\n",
      "        -0.3660,  0.3576,  0.3693,  0.3104, -0.3673, -0.2656, -0.3372, -0.3174],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6600, 1.3408, 0.7065, 0.6595, 0.6598, 0.6601, 1.3406, 0.6590, 0.7383,\n",
      "        0.6588, 1.3381, 1.3404, 1.3409, 1.3405, 1.3419, 0.6597],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3406,  0.2901,  0.3399,  0.3404,  0.3405, -0.3411,  0.3405, -0.3396,\n",
      "        -0.3364,  0.3411,  0.3401,  0.3403, -0.3406,  0.3406, -0.3408, -0.2785],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6578, 1.1355, 0.7134, 0.6578, 0.6692, 0.7842, 0.8219, 0.6572, 0.6565,\n",
      "        0.7118, 0.7964, 0.9536, 0.6801, 0.8678, 0.7379, 0.6745],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1210,  0.0983,  0.2437,  0.2677,  0.0152,  0.1962,  0.1306,  0.1566,\n",
      "         0.3395,  0.0428, -0.0504,  0.3353, -0.0448,  0.3023,  0.2233,  0.3409],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0989,  0.3583,  0.1686, -0.0036, -0.2991, -0.0244, -0.4259,  0.0075,\n",
      "          0.0083,  0.6173,  0.4592,  0.1109,  0.5706,  0.4850, -0.2319,  0.1839],\n",
      "        [-0.3551, -0.4712,  0.1791,  0.4153,  0.2371, -0.6300,  0.2784,  0.3722,\n",
      "          0.5697, -0.3535,  0.0550, -0.2520, -0.0174, -0.2401,  0.5823, -0.2661],\n",
      "        [ 0.0348,  0.5340, -0.1074, -0.5532, -0.0366,  0.3862, -0.3988, -0.0766,\n",
      "          0.1811,  0.2098,  0.3660, -0.1933,  0.1649,  0.3152, -0.5600,  0.3773],\n",
      "        [-0.0109, -0.4107,  0.0369, -0.0193,  0.0212, -0.1899,  0.0816,  0.5737,\n",
      "          0.4285,  0.0878, -0.0228, -0.2638,  0.2310,  0.3247, -0.0155,  0.0541],\n",
      "        [-0.3371, -0.0119, -0.1971,  0.5936,  0.3124, -0.3953,  0.5150,  0.2910,\n",
      "          0.3614, -0.3386, -0.3020,  0.0611, -0.0182, -0.1831,  0.3672, -0.2531],\n",
      "        [-0.0048,  0.1707, -0.2930, -0.1407,  0.0248,  0.3322, -0.4059, -0.1804,\n",
      "         -0.1255,  0.4852,  0.1572, -0.0591,  0.1774,  0.0028,  0.0223,  0.5314],\n",
      "        [ 0.3464,  0.1839, -0.2140, -0.5104,  0.1656,  0.4436, -0.3396, -0.3137,\n",
      "         -0.3842,  0.1427, -0.2464,  0.1456,  0.0571, -0.0086, -0.5027,  0.6416],\n",
      "        [ 0.1319, -0.0346,  0.3846,  0.4506,  0.0949,  0.3733,  0.3752, -0.3197,\n",
      "         -0.4248, -0.0258,  0.0199,  0.2001, -0.3517, -0.1333, -0.1446, -0.3170],\n",
      "        [ 0.4166, -0.2448,  0.6278, -0.1685,  0.3511,  0.1651, -0.1240, -0.0832,\n",
      "         -0.5893, -0.2242, -0.1856,  0.2970, -0.2304, -0.2719, -0.3210, -0.1366],\n",
      "        [ 0.2029,  0.0371,  0.1041, -0.1212,  0.2922, -0.0195, -0.1635,  0.3512,\n",
      "         -0.3794, -0.6330, -0.4608,  0.4081, -0.2258, -0.0152, -0.0550, -0.0745],\n",
      "        [ 0.6130,  0.1424,  0.3831,  0.2234,  0.2818, -0.3449,  0.1052, -0.0176,\n",
      "         -0.1998, -0.2222, -0.5302,  0.3971, -0.5744,  0.0352, -0.3314, -0.3901],\n",
      "        [-0.1021, -0.1596, -0.0771, -0.0622, -0.2205, -0.2141,  0.0963, -0.1038,\n",
      "          0.4684,  0.4934,  0.4131, -0.5370,  0.1724, -0.2216,  0.4519, -0.1075],\n",
      "        [ 0.3471, -0.1247,  0.5209, -0.0761,  0.1549,  0.0679, -0.1265,  0.0039,\n",
      "         -0.5520, -0.1278, -0.6266,  0.3233,  0.0689, -0.0269, -0.2825,  0.1911],\n",
      "        [-0.3412, -0.1813, -0.5994, -0.4016, -0.4108,  0.1544,  0.1286,  0.0153,\n",
      "          0.4922,  0.1201,  0.3420, -0.3052,  0.4760,  0.4615,  0.4558,  0.2488],\n",
      "        [-0.1890,  0.0226, -0.6300, -0.1125, -0.5948,  0.0772, -0.3295, -0.2478,\n",
      "          0.1616,  0.4915,  0.1295, -0.2293,  0.1963,  0.2103,  0.6223,  0.1973],\n",
      "        [-0.3312,  0.2663, -0.1590, -0.1702,  0.1283,  0.5788, -0.3312, -0.2511,\n",
      "          0.4051,  0.3594,  0.2088, -0.3176,  0.3369,  0.1806, -0.3095,  0.3380],\n",
      "        [ 0.0128, -0.1040, -0.3346,  0.1907, -0.1231, -0.3846, -0.0940, -0.0059,\n",
      "          0.4891,  0.1122,  0.3753, -0.2865,  0.2038,  0.5360,  0.3344,  0.1354],\n",
      "        [ 0.3621,  0.1158,  0.4914, -0.5402,  0.2939,  0.6216, -0.1074, -0.0102,\n",
      "         -0.3862,  0.1444, -0.5936,  0.0720, -0.1172,  0.3128, -0.5384,  0.6360],\n",
      "        [-0.0845,  0.0676,  0.0955,  0.5166,  0.2098, -0.4743,  0.1887,  0.1809,\n",
      "          0.3231, -0.0380,  0.0770, -0.3835,  0.1144,  0.2076,  0.2225, -0.3864],\n",
      "        [-0.4223,  0.2583, -0.1083, -0.0736,  0.4133,  0.1165,  0.1999, -0.3335,\n",
      "         -0.0467, -0.4687,  0.0805, -0.0056, -0.4832, -0.2805, -0.3988, -0.2281],\n",
      "        [ 0.3601,  0.1972,  0.4160, -0.1372,  0.1247,  0.2462, -0.6298, -0.1148,\n",
      "         -0.3870,  0.2201, -0.5564,  0.4936, -0.1524,  0.1401, -0.1211,  0.3600],\n",
      "        [-0.5916, -0.0475, -0.2712,  0.1783, -0.5197, -0.0415,  0.4033,  0.3582,\n",
      "          0.5375, -0.1663,  0.4486, -0.2790,  0.1384, -0.1977,  0.0904, -0.4331],\n",
      "        [-0.6114, -0.0941, -0.1596,  0.3989, -0.3896, -0.6242,  0.3979,  0.5289,\n",
      "          0.5358,  0.1904,  0.2685, -0.4310,  0.0803,  0.0618,  0.6033, -0.2475],\n",
      "        [ 0.1953, -0.0256,  0.2029, -0.0569, -0.3153,  0.0617, -0.3785, -0.3004,\n",
      "          0.0132, -0.1157, -0.1595,  0.2266,  0.1833, -0.0320, -0.2268,  0.0661],\n",
      "        [-0.5715, -0.0662, -0.6432,  0.2675, -0.3208, -0.0566,  0.5246,  0.5176,\n",
      "          0.6211,  0.3254,  0.2592, -0.2537, -0.1559, -0.1800,  0.4538, -0.3661],\n",
      "        [-0.5184, -0.2677, -0.2250,  0.4295, -0.4107, -0.5433,  0.0327,  0.4958,\n",
      "          0.2462,  0.4976,  0.3655, -0.2051,  0.1355, -0.1749,  0.4160, -0.3800],\n",
      "        [-0.1578, -0.5322, -0.3646,  0.4159, -0.4831, -0.1384,  0.0833,  0.2054,\n",
      "          0.5792,  0.5198,  0.4323, -0.2913,  0.4530, -0.0172,  0.4636, -0.4007],\n",
      "        [ 0.0911,  0.2123,  0.0458, -0.6348,  0.3063,  0.5967, -0.3132, -0.1517,\n",
      "         -0.5674, -0.1248, -0.5125,  0.5633,  0.5056,  0.5732, -0.4893,  0.4938],\n",
      "        [-0.5219, -0.4889, -0.6231,  0.1590, -0.1314, -0.4664,  0.3998,  0.3661,\n",
      "          0.2877, -0.0313,  0.0912, -0.3160, -0.5458, -0.3381,  0.6008, -0.5137],\n",
      "        [ 0.0767,  0.0650,  0.3875, -0.5739,  0.5919,  0.4356, -0.6282, -0.3011,\n",
      "         -0.5399, -0.1046, -0.3031,  0.2128, -0.4376, -0.4084, -0.5556,  0.0607],\n",
      "        [-0.3200,  0.4080,  0.4695, -0.3949, -0.0346,  0.4128,  0.3699, -0.0064,\n",
      "         -0.1606, -0.5962, -0.5190, -0.0408, -0.5844, -0.4407, -0.3115,  0.0699],\n",
      "        [ 0.0596,  0.1612,  0.6324, -0.0523,  0.6028,  0.0491, -0.3010, -0.1028,\n",
      "         -0.6401, -0.1545, -0.1762,  0.3428, -0.0012, -0.2948, -0.4083,  0.5381],\n",
      "        [ 0.5536, -0.0718,  0.3141,  0.5954,  0.3054, -0.0846, -0.0154,  0.4180,\n",
      "         -0.2001, -0.3444, -0.4980,  0.0426, -0.0928, -0.1497, -0.3573, -0.2045],\n",
      "        [-0.3394,  0.4630, -0.3516, -0.2957, -0.1088, -0.1150,  0.6230, -0.0379,\n",
      "          0.0818,  0.3623,  0.4309, -0.2396, -0.1119, -0.1690,  0.4872, -0.5260],\n",
      "        [ 0.1856,  0.3118,  0.3821,  0.6223,  0.6313,  0.4806, -0.1969,  0.4838,\n",
      "         -0.3353, -0.1594, -0.0771,  0.2680, -0.1966,  0.4331, -0.2675,  0.1655],\n",
      "        [ 0.1918, -0.3937,  0.0632,  0.1015,  0.1497,  0.0664, -0.4068,  0.4597,\n",
      "         -0.6035, -0.5392, -0.6341,  0.6190,  0.2127,  0.0968, -0.2069,  0.5184],\n",
      "        [-0.0696,  0.0703, -0.2500, -0.2523, -0.2671,  0.3151,  0.6123, -0.5100,\n",
      "          0.0872, -0.2440,  0.4127, -0.3183,  0.2128, -0.5519,  0.3642, -0.0903],\n",
      "        [-0.2983, -0.2088, -0.3113, -0.3922, -0.2807, -0.5584,  0.0383, -0.4689,\n",
      "          0.1527, -0.2482,  0.0522, -0.4786, -0.0846, -0.4948,  0.3077, -0.4757],\n",
      "        [ 0.2499,  0.0708, -0.2802,  0.1350,  0.3125,  0.0047, -0.5841,  0.4005,\n",
      "          0.2618, -0.1484, -0.4849,  0.1121, -0.1035,  0.0086, -0.1698, -0.0115],\n",
      "        [ 0.4130,  0.1594,  0.1566, -0.2029,  0.1857,  0.3665, -0.0568, -0.4838,\n",
      "         -0.0620, -0.3076,  0.1580,  0.3801,  0.0655, -0.3747,  0.0456,  0.3169],\n",
      "        [ 0.4485, -0.0513,  0.2373,  0.1010,  0.0578,  0.3578, -0.5025,  0.6045,\n",
      "         -0.4762, -0.6309, -0.4474,  0.1574, -0.0642,  0.3208, -0.0987,  0.6279],\n",
      "        [-0.3069,  0.0178, -0.2605,  0.0632, -0.3425, -0.0923,  0.0888,  0.3599,\n",
      "          0.1265,  0.1566,  0.2421, -0.0783, -0.1172,  0.0411,  0.1373, -0.3337],\n",
      "        [ 0.1683, -0.1999, -0.0617,  0.2274,  0.3491,  0.0466, -0.2228,  0.3284,\n",
      "         -0.2070, -0.1615, -0.3603, -0.1701,  0.2071,  0.1015,  0.2293,  0.3647],\n",
      "        [-0.0298,  0.4906,  0.0104, -0.3371, -0.1707,  0.4760, -0.0305, -0.2125,\n",
      "         -0.5417, -0.1354,  0.2145,  0.0219,  0.0649,  0.1975, -0.0156,  0.2176],\n",
      "        [ 0.3510,  0.5065,  0.5209, -0.2178,  0.3974,  0.3306, -0.4006, -0.3432,\n",
      "         -0.0891, -0.0436, -0.4341,  0.1147,  0.0648, -0.4457, -0.0511,  0.3051],\n",
      "        [-0.3429,  0.3459, -0.2427, -0.3143, -0.1848, -0.4170,  0.1781, -0.3860,\n",
      "          0.4335,  0.1023,  0.0593, -0.5513,  0.1695, -0.4918,  0.4930, -0.0587],\n",
      "        [-0.1195, -0.1762, -0.4118, -0.0495, -0.1396, -0.1100,  0.2177,  0.5262,\n",
      "          0.1554,  0.2302,  0.5665, -0.2153,  0.0111, -0.4220,  0.1762, -0.3746],\n",
      "        [ 0.5256,  0.0507, -0.0179,  0.1137,  0.6057, -0.1683, -0.2846,  0.2660,\n",
      "          0.3443,  0.3682, -0.5508,  0.4011,  0.4413,  0.3446, -0.0967,  0.5218]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3432, -0.3399,  0.3160, -0.0995, -0.3258,  0.2685,  0.3282, -0.3166,\n",
      "        -0.3435, -0.3437, -0.2838,  0.3431, -0.3432,  0.3388,  0.3442,  0.2097,\n",
      "         0.0045,  0.1406, -0.0166, -0.1451,  0.1212, -0.0780, -0.1495,  0.0489,\n",
      "        -0.1486, -0.1265, -0.1504,  0.1692, -0.1195,  0.1525,  0.1560,  0.0645,\n",
      "        -0.3231,  0.3402, -0.3415, -0.3410,  0.3399,  0.3400, -0.2894,  0.1576,\n",
      "        -0.3407, -0.2498, -0.3400,  0.3145,  0.2449,  0.3415,  0.3403, -0.3404],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1362,  0.0559, -0.3783, -0.1420, -0.1518,  0.2643, -0.5245, -0.2098,\n",
      "         -0.3512, -0.1154, -0.5575, -0.5180,  0.4147,  0.3383,  0.5123,  0.2774],\n",
      "        [ 0.0889,  0.1291, -0.0859,  0.0888,  0.1133,  0.2912, -0.1955, -0.4201,\n",
      "          0.1540,  0.2298,  0.0309,  0.2284,  0.1060, -0.0559, -0.1389,  0.2314],\n",
      "        [-0.0939,  0.4856,  0.1246, -0.2877,  0.4193, -0.1429,  0.1154,  0.3133,\n",
      "          0.2919, -0.0121,  0.1872,  0.1465,  0.0521, -0.1525, -0.1060, -0.3839],\n",
      "        [ 0.0554,  0.0059, -0.3420, -0.0885,  0.0628,  0.2647, -0.0833,  0.2548,\n",
      "         -0.1338, -0.2243, -0.3161, -0.0887,  0.1912,  0.1264,  0.4700,  0.0801],\n",
      "        [-0.4697,  0.2817,  0.1077, -0.2874,  0.5143,  0.2199,  0.2024,  0.5015,\n",
      "          0.4179,  0.2535,  0.2011,  0.2186,  0.2245,  0.2356, -0.3144, -0.3657],\n",
      "        [-0.2941,  0.1551, -0.5371, -0.5086,  0.3948,  0.2424, -0.1803, -0.4805,\n",
      "         -0.2370,  0.3391, -0.2369, -0.1752,  0.4782,  0.1709,  0.4893, -0.2501],\n",
      "        [ 0.4909,  0.0177, -0.1859, -0.1014,  0.0196,  0.5285, -0.1398, -0.3712,\n",
      "         -0.2745, -0.5003, -0.2570, -0.5258,  0.2853,  0.3095,  0.1736,  0.4963],\n",
      "        [-0.0696, -0.2863,  0.4271,  0.0776, -0.3004, -0.1019,  0.3756,  0.2030,\n",
      "         -0.1163, -0.2788,  0.2645,  0.1801, -0.2804, -0.0991, -0.1569, -0.3426],\n",
      "        [-0.1397, -0.2418,  0.1351,  0.2905, -0.1944, -0.2295,  0.2607,  0.1595,\n",
      "          0.1524, -0.1040,  0.1365,  0.1311, -0.1958, -0.2274, -0.1606, -0.3815],\n",
      "        [ 0.2841, -0.4066, -0.4178,  0.2512, -0.3360,  0.0912, -0.5255, -0.5572,\n",
      "         -0.4522,  0.0757, -0.2356, -0.5088,  0.5868,  0.2909,  0.3323,  0.3078],\n",
      "        [-0.4595, -0.0177,  0.2638, -0.3327, -0.0021, -0.0707,  0.0931,  0.1925,\n",
      "          0.2521,  0.2758,  0.4385, -0.1044, -0.2403, -0.4316, -0.5348, -0.3061],\n",
      "        [-0.5605,  0.0183,  0.3832,  0.0896,  0.1126,  0.2679,  0.1108,  0.4243,\n",
      "          0.1815,  0.4526,  0.2134, -0.1295, -0.3673, -0.4080, -0.2527, -0.2497],\n",
      "        [-0.4095,  0.3097,  0.0897, -0.2388,  0.4214,  0.0049,  0.2131,  0.4772,\n",
      "          0.3158,  0.2531,  0.0556,  0.1476, -0.2601, -0.0693, -0.1099, -0.3812],\n",
      "        [ 0.0832,  0.2032,  0.5440,  0.2350, -0.0333, -0.2469,  0.2298,  0.2327,\n",
      "          0.2737, -0.3972, -0.0147,  0.2457, -0.2179, -0.4938, -0.1061, -0.0903],\n",
      "        [ 0.5161, -0.2650, -0.3126,  0.4304, -0.3316, -0.1071, -0.1783, -0.1990,\n",
      "         -0.0155,  0.0398,  0.3330, -0.1371, -0.1921, -0.5533,  0.3898,  0.3548],\n",
      "        [-0.1613,  0.0382, -0.3361, -0.0086, -0.0027,  0.3485,  0.2207,  0.0317,\n",
      "          0.2375,  0.1186, -0.1807, -0.0654, -0.3482,  0.0220, -0.3481, -0.2455]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3397,  0.3354,  0.3414, -0.3410,  0.3422,  0.2161, -0.3410, -0.1885,\n",
      "         0.3386, -0.3408,  0.3409,  0.3394,  0.3416,  0.0788, -0.2899,  0.3409],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.9580e-01,  3.5915e-01, -5.8660e-02, -1.5414e-01, -6.7091e-02,\n",
      "          4.1606e-01,  5.6947e-02, -3.2193e-01, -3.3426e-01, -1.7062e-01,\n",
      "          6.1457e-01,  7.4296e-02,  5.4068e-01, -4.3390e-01,  2.4239e-01,\n",
      "          2.9405e-01],\n",
      "        [-2.0511e-01,  5.6029e-02, -6.0889e-01, -2.3176e-01, -1.6425e-01,\n",
      "         -2.6616e-01,  4.5815e-01, -3.1487e-01,  3.3502e-01,  5.0708e-01,\n",
      "          4.4518e-01, -4.2889e-01,  5.0651e-01, -3.7278e-02,  1.0615e-01,\n",
      "         -7.1221e-02],\n",
      "        [ 1.0131e-01,  4.7507e-01, -1.2741e-01, -1.3267e-01,  3.4608e-01,\n",
      "          3.0023e-01, -3.9540e-02, -1.7474e-01, -6.8962e-02,  3.9031e-01,\n",
      "         -1.9682e-01,  1.9719e-02,  3.6056e-02,  4.5063e-01, -6.1010e-01,\n",
      "          6.4259e-01],\n",
      "        [-2.1000e-01, -1.7642e-01,  3.3515e-02, -2.8057e-01, -4.7247e-01,\n",
      "         -5.6789e-01,  3.8868e-01, -3.4065e-02,  1.0346e-01, -7.0195e-02,\n",
      "          5.5142e-01, -1.6451e-02,  2.5711e-01, -7.1848e-02,  1.4386e-01,\n",
      "         -4.0122e-01],\n",
      "        [ 1.7309e-01,  2.5399e-01,  5.6272e-01,  2.4932e-02,  4.1392e-01,\n",
      "          3.7145e-02, -6.8293e-02,  8.1816e-02, -1.6750e-01, -4.6565e-01,\n",
      "         -2.5084e-01, -3.4612e-02,  4.9061e-02,  4.2188e-01, -3.4800e-01,\n",
      "          4.0114e-01],\n",
      "        [-5.1492e-02, -3.4509e-01, -7.1017e-02,  3.1401e-01, -1.4527e-01,\n",
      "         -2.9107e-01,  2.6658e-01,  1.7081e-01,  1.8859e-05,  4.9691e-02,\n",
      "          2.2277e-01, -1.1945e-01, -5.3238e-02,  3.3548e-01,  3.2319e-01,\n",
      "         -2.8111e-01],\n",
      "        [-5.5041e-01, -8.1063e-02,  7.4617e-02,  3.9036e-01,  6.7008e-02,\n",
      "         -1.9914e-01,  1.5970e-01,  1.1690e-01,  2.3782e-01, -4.2484e-02,\n",
      "          8.0533e-02, -2.9948e-03, -2.0041e-01,  7.4850e-02,  3.1875e-01,\n",
      "         -4.2981e-01],\n",
      "        [-5.7034e-01, -1.4533e-01, -3.2918e-01,  3.8291e-01, -5.0036e-01,\n",
      "         -1.5033e-01,  1.8920e-01, -2.8369e-01,  1.9404e-01,  2.3293e-01,\n",
      "          6.1776e-01, -5.8819e-01,  3.3352e-02, -4.1431e-01,  3.0834e-01,\n",
      "         -5.1803e-01],\n",
      "        [-5.7333e-01, -3.0431e-01,  3.1126e-01,  2.0875e-01, -4.4074e-01,\n",
      "          9.8782e-02,  3.1161e-01, -9.9475e-02,  4.2332e-01, -8.8575e-02,\n",
      "          5.1904e-01, -2.7536e-01, -2.0713e-01, -4.9387e-01,  4.6778e-01,\n",
      "         -9.9043e-02],\n",
      "        [ 1.0811e-01,  3.8720e-01, -1.7007e-01, -1.7185e-01,  3.4866e-01,\n",
      "          2.9365e-02, -4.0996e-01, -2.0941e-01, -5.4547e-01,  5.3147e-02,\n",
      "         -4.7515e-01, -1.7661e-01,  2.5679e-01,  4.8605e-01,  1.2113e-02,\n",
      "          4.8272e-01],\n",
      "        [ 3.7029e-01, -3.2598e-02, -1.3674e-01,  5.8303e-02,  1.2173e-01,\n",
      "          3.0116e-01,  3.0896e-01, -8.4508e-02, -4.7731e-01, -7.4532e-03,\n",
      "         -2.7879e-01,  1.0256e-01, -2.5444e-01, -1.6433e-01,  6.4782e-02,\n",
      "          1.3113e-01],\n",
      "        [ 1.0518e-01,  6.3825e-01, -1.8887e-01, -9.4037e-02,  7.6774e-02,\n",
      "          6.0972e-01, -3.6624e-01, -5.2480e-01, -4.8954e-01, -3.5718e-02,\n",
      "          2.4195e-03, -1.5405e-01,  6.0233e-02,  4.8384e-01, -2.4334e-01,\n",
      "          1.7268e-01],\n",
      "        [ 1.9000e-01, -3.5554e-01, -2.9754e-01, -2.9977e-01,  1.7797e-01,\n",
      "          2.5400e-01, -4.6057e-01, -1.9050e-01,  2.4277e-01,  1.4116e-01,\n",
      "         -6.2714e-01,  8.4676e-02,  1.2389e-01,  1.2420e-01, -2.4285e-01,\n",
      "          3.0672e-01],\n",
      "        [-2.0634e-01,  4.1691e-04, -6.6209e-02,  3.0297e-01, -6.0640e-01,\n",
      "         -3.9272e-01,  9.0882e-02,  5.8937e-02,  6.1077e-01, -1.5127e-01,\n",
      "          3.5018e-01, -4.1690e-01, -1.0977e-01, -5.8438e-01,  2.6550e-01,\n",
      "         -4.8667e-01],\n",
      "        [-1.6845e-01,  1.6884e-02,  3.5024e-01,  3.8761e-01,  2.2705e-01,\n",
      "         -3.9809e-01,  4.2997e-01,  2.4978e-01,  5.1467e-01, -5.3606e-01,\n",
      "         -1.7250e-01,  4.0497e-01,  8.4578e-02,  2.9748e-01, -1.6215e-01,\n",
      "         -3.1328e-01],\n",
      "        [-5.8448e-01, -4.7961e-01,  4.4918e-01,  6.1200e-01, -8.4946e-02,\n",
      "         -2.2530e-01,  2.6392e-01,  3.0819e-01,  3.4955e-01, -2.2466e-01,\n",
      "         -1.0629e-01,  5.2624e-02, -1.7853e-01,  4.9842e-01,  4.6337e-01,\n",
      "         -5.5308e-01],\n",
      "        [-1.6591e-01, -4.8696e-01, -4.3010e-01,  5.0780e-01, -1.0284e-01,\n",
      "         -3.2308e-01, -2.8230e-02,  2.5442e-01,  3.0991e-01,  5.0568e-01,\n",
      "          4.3213e-01, -2.0799e-01,  3.9296e-01,  7.9254e-02,  3.1803e-01,\n",
      "         -8.2319e-03],\n",
      "        [ 1.6426e-01, -4.2960e-01,  4.9898e-01, -2.7609e-01,  4.2068e-01,\n",
      "          1.4769e-01,  6.7739e-02, -5.5560e-01, -2.7377e-01,  1.5828e-01,\n",
      "         -1.5191e-01,  6.7682e-02, -2.6988e-03, -9.1809e-02, -8.8810e-02,\n",
      "         -4.8051e-01],\n",
      "        [-4.6076e-02,  1.8437e-01, -4.5797e-01,  1.7189e-01, -7.5044e-02,\n",
      "         -4.1120e-01, -1.3081e-01,  9.0496e-02,  1.1914e-02, -1.5405e-01,\n",
      "          9.3407e-02, -1.5096e-02,  1.9507e-01,  5.5798e-01,  2.4620e-01,\n",
      "         -3.2306e-01],\n",
      "        [ 1.3040e-01,  9.5493e-02, -5.0133e-01,  6.0005e-02, -5.2089e-01,\n",
      "          3.9092e-01, -3.2949e-01, -1.9125e-01, -5.3391e-01, -6.0537e-01,\n",
      "          1.0215e-02, -1.5081e-01, -3.3039e-01, -4.8170e-01,  2.4023e-01,\n",
      "          4.2523e-01],\n",
      "        [-3.6136e-01, -1.9574e-01,  2.9488e-01, -3.0147e-02,  1.8038e-01,\n",
      "         -5.6437e-01, -1.0312e-02,  2.8414e-01,  2.8716e-01,  2.6076e-01,\n",
      "          4.6023e-01, -2.9285e-01,  2.6265e-01,  4.9210e-02, -4.1188e-02,\n",
      "         -2.3632e-01],\n",
      "        [ 4.5201e-02,  5.5153e-01,  6.1498e-01, -2.6678e-01,  1.9225e-01,\n",
      "          1.5012e-01, -3.4848e-01, -1.9306e-01, -1.3334e-01, -3.1861e-01,\n",
      "         -5.8194e-01,  3.5291e-01, -5.2241e-02, -1.7575e-01, -2.4338e-01,\n",
      "          4.2121e-01],\n",
      "        [-1.5892e-01, -3.2547e-01,  4.2416e-01,  5.9387e-01,  8.2959e-02,\n",
      "         -1.7470e-01, -2.0500e-01,  2.3405e-01,  7.4305e-02,  2.2295e-01,\n",
      "          3.3195e-01, -8.3461e-02,  3.3806e-01,  5.1294e-01, -2.9213e-02,\n",
      "         -5.6675e-01],\n",
      "        [-2.2668e-01, -4.5828e-01,  5.3024e-01,  5.0810e-01,  5.2453e-01,\n",
      "         -5.2241e-01,  3.1073e-01,  1.4491e-01,  1.4367e-01,  2.7112e-01,\n",
      "          1.7509e-01, -2.0835e-01,  2.3061e-01,  6.2280e-01,  1.2365e-01,\n",
      "         -4.5191e-01],\n",
      "        [-2.2827e-01,  4.9845e-01, -5.5247e-01,  4.9602e-01,  5.5352e-01,\n",
      "         -1.5290e-01,  3.4091e-01, -4.1993e-01, -4.1599e-01, -1.8942e-02,\n",
      "          6.1021e-01,  1.7191e-01,  9.7037e-03,  8.0835e-02, -3.5504e-01,\n",
      "          1.5350e-01],\n",
      "        [-8.9601e-02, -4.0832e-01,  6.2195e-01,  3.6104e-01,  5.7786e-01,\n",
      "         -3.8078e-01, -3.1084e-01,  2.7671e-01,  1.4402e-01,  4.6870e-01,\n",
      "         -2.3986e-01, -1.1182e-01,  1.4974e-01,  2.2358e-01, -9.6969e-02,\n",
      "         -3.3496e-01],\n",
      "        [ 3.3441e-01, -1.5923e-01,  4.7633e-01, -5.0454e-02, -4.1064e-01,\n",
      "          4.1243e-01, -2.7343e-01,  6.1895e-01, -6.7254e-03, -7.1171e-02,\n",
      "         -1.2418e-01, -1.6204e-01, -1.3215e-02, -2.8549e-01,  2.4200e-01,\n",
      "         -3.9167e-01],\n",
      "        [-2.0506e-01,  5.7003e-02, -3.7384e-01,  3.5729e-01,  8.4076e-02,\n",
      "         -5.8768e-01,  5.7631e-01, -4.4886e-02,  5.4933e-01, -3.0308e-01,\n",
      "         -4.3737e-02,  2.7083e-01,  2.0747e-01, -3.3484e-01, -3.6523e-01,\n",
      "          2.5168e-01],\n",
      "        [ 5.1200e-01, -6.2686e-01,  3.8507e-01, -5.9517e-01, -3.5786e-01,\n",
      "          3.9564e-02, -1.0874e-01,  1.6670e-01,  8.7093e-02,  1.1068e-01,\n",
      "         -1.1171e-01, -7.0388e-02,  1.3456e-01, -2.3725e-01,  9.3745e-02,\n",
      "         -3.1938e-01],\n",
      "        [-4.1080e-01,  5.6269e-01, -4.5881e-01, -2.6437e-02, -1.7678e-01,\n",
      "         -5.1139e-01,  1.4649e-01, -4.0105e-01, -2.5920e-01,  2.3328e-02,\n",
      "          2.2884e-01, -7.8180e-02, -6.2861e-02, -1.3438e-01, -8.0960e-02,\n",
      "         -2.3193e-01],\n",
      "        [ 3.6142e-01,  5.0387e-01, -6.1894e-02, -5.9344e-01, -2.0439e-01,\n",
      "         -2.5157e-01,  3.8733e-01,  1.5999e-02,  3.0269e-01, -5.3911e-01,\n",
      "          9.8703e-02,  2.0073e-01,  1.5485e-01, -4.8406e-01,  1.0396e-01,\n",
      "          3.3535e-01],\n",
      "        [-4.0808e-01,  5.5822e-01, -2.3560e-01, -3.7805e-01,  6.4831e-02,\n",
      "         -1.1232e-01, -1.8900e-01, -5.3327e-01,  2.9300e-02,  1.5776e-01,\n",
      "          4.3839e-01, -2.2057e-01, -2.2027e-01, -5.2481e-01, -2.7703e-01,\n",
      "          3.6350e-01],\n",
      "        [-5.3536e-02, -4.1128e-01, -2.0735e-01, -3.1363e-01, -8.8020e-02,\n",
      "          6.5268e-02, -2.0916e-02,  1.6628e-02,  2.1902e-01, -3.8712e-01,\n",
      "          8.3861e-02,  2.0041e-01,  5.3120e-01,  7.1440e-02,  2.6587e-01,\n",
      "          4.5629e-01],\n",
      "        [ 4.5828e-01, -2.5221e-01, -3.8352e-01, -1.1255e-01, -2.9274e-01,\n",
      "          3.1958e-01, -4.4566e-01,  1.6345e-01,  4.3671e-02, -2.8119e-01,\n",
      "         -3.3539e-01, -1.7808e-01,  3.1532e-02,  3.9000e-01,  2.6787e-01,\n",
      "          1.7029e-01],\n",
      "        [ 2.4637e-01, -4.4176e-01,  4.8612e-01, -7.7482e-02, -1.2299e-01,\n",
      "          1.4135e-01,  1.5838e-01, -8.7457e-02,  6.9154e-02, -7.7980e-02,\n",
      "          1.9128e-02,  5.5589e-01, -3.6003e-01,  3.3897e-01, -2.7860e-01,\n",
      "          1.8201e-01],\n",
      "        [ 2.3082e-01,  3.5888e-01,  2.4492e-01, -1.7782e-01,  4.8408e-02,\n",
      "         -5.8796e-02, -4.2066e-01, -2.0749e-02,  1.0711e-01,  6.1050e-01,\n",
      "          8.7944e-02, -3.8593e-01,  1.6433e-01, -4.4040e-01,  7.7530e-02,\n",
      "         -4.8461e-01],\n",
      "        [ 5.8104e-02, -4.8107e-01,  1.1805e-01, -3.6025e-01,  3.9492e-02,\n",
      "         -1.2885e-01,  2.9215e-01, -1.4627e-01, -3.8507e-01, -6.5158e-02,\n",
      "          4.3860e-02,  9.0917e-02, -1.7098e-01,  4.8931e-01, -2.0383e-01,\n",
      "         -2.1197e-02],\n",
      "        [-4.1389e-02,  9.7691e-02,  1.6871e-01,  6.4612e-02,  2.4903e-01,\n",
      "         -2.6874e-02,  1.2509e-01, -1.6675e-01, -4.7754e-01,  5.6989e-01,\n",
      "         -3.5939e-01,  1.9322e-01, -3.4726e-02, -5.2929e-01, -4.5682e-01,\n",
      "         -1.7803e-01],\n",
      "        [ 2.7141e-01,  5.4940e-01,  1.0864e-01,  7.0667e-02,  1.7091e-02,\n",
      "          9.8744e-02, -6.3169e-02, -2.7600e-01, -4.4948e-02,  5.3832e-02,\n",
      "         -8.7713e-02, -2.3034e-01, -1.5111e-01, -3.3527e-01,  3.6008e-02,\n",
      "         -8.7740e-02],\n",
      "        [-9.5149e-03,  4.6427e-01,  1.4476e-01, -2.4039e-01, -1.9697e-01,\n",
      "          4.6233e-01,  1.5235e-01, -1.2624e-01,  3.8705e-01, -4.1170e-02,\n",
      "         -1.8441e-01, -2.0971e-01, -2.8481e-01, -3.3455e-02,  2.0510e-01,\n",
      "         -6.0787e-01],\n",
      "        [-2.6185e-02, -6.0026e-01,  3.2427e-01,  1.2882e-01, -9.7721e-02,\n",
      "          1.2267e-01,  2.9185e-02,  4.0521e-02,  4.3272e-02, -4.4917e-01,\n",
      "         -9.6585e-02,  4.7661e-01, -2.2527e-01,  1.6200e-01, -5.5225e-04,\n",
      "          4.7727e-01],\n",
      "        [ 1.6960e-01,  1.6094e-01, -4.9534e-01, -6.1812e-02, -2.8883e-01,\n",
      "          3.8353e-01,  1.5961e-02,  3.8777e-01,  2.3041e-01, -5.6614e-03,\n",
      "          1.2434e-02, -5.1057e-01,  2.2794e-01, -7.3018e-02,  5.4358e-02,\n",
      "          2.5499e-01],\n",
      "        [-3.1504e-01,  3.0578e-01,  1.4769e-01,  9.2367e-02,  1.8484e-01,\n",
      "         -3.1267e-01,  1.9582e-01, -4.7689e-01, -1.1161e-01, -6.2210e-02,\n",
      "          4.0278e-01,  4.2615e-01, -2.5298e-01,  1.2963e-01, -1.3960e-01,\n",
      "         -1.4664e-01],\n",
      "        [-1.2750e-01, -2.1196e-01,  2.6400e-01, -5.3625e-02, -2.9226e-01,\n",
      "          2.1202e-02,  5.1495e-01, -3.4220e-01,  3.5326e-01, -2.3704e-01,\n",
      "         -2.0980e-03,  3.4241e-01,  6.8825e-02,  7.6820e-02, -1.1321e-01,\n",
      "          3.9854e-01],\n",
      "        [-2.5050e-01,  3.7214e-02, -4.2446e-01,  3.4844e-01,  2.6068e-01,\n",
      "         -3.4614e-01, -5.5981e-01,  2.3908e-01, -2.0435e-01,  4.8134e-01,\n",
      "         -1.8319e-01, -5.6996e-01,  1.6854e-01, -4.8276e-01,  5.7065e-01,\n",
      "         -3.9047e-01],\n",
      "        [ 2.4900e-01, -3.0191e-01,  5.0173e-01, -4.5676e-01, -6.3984e-02,\n",
      "          2.4812e-01,  4.3665e-01, -3.3289e-01,  1.9150e-02, -2.2958e-01,\n",
      "         -5.7978e-01,  3.3334e-01, -5.7873e-02,  4.4496e-01, -2.1911e-01,\n",
      "          3.5686e-01],\n",
      "        [-2.0380e-02,  1.2709e-01,  3.7552e-01,  1.8321e-01, -1.3515e-01,\n",
      "         -5.3897e-02, -1.4643e-01, -2.7919e-01,  6.7730e-02,  6.1720e-01,\n",
      "         -2.4486e-01,  5.3165e-02, -2.0600e-01, -4.5202e-01,  2.0258e-02,\n",
      "         -3.1565e-01],\n",
      "        [-3.5710e-01,  4.5126e-01, -1.9171e-01,  1.6820e-01,  3.9277e-02,\n",
      "         -1.3679e-01,  2.5801e-01,  2.3415e-01, -4.8997e-01,  2.9231e-01,\n",
      "          1.7105e-01,  8.6796e-02,  6.2741e-02, -6.3243e-01,  1.6901e-02,\n",
      "         -2.0219e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3414,  0.3408, -0.3370,  0.3396, -0.3405, -0.0058, -0.0352,  0.3403,\n",
      "         0.2844, -0.3415, -0.1831,  0.2557, -0.3403,  0.3402, -0.1764, -0.3406,\n",
      "        -0.0085, -0.0160,  0.1024, -0.0238, -0.0094, -0.0044, -0.0791, -0.0543,\n",
      "        -0.0147, -0.0490,  0.0295,  0.0149,  0.0200, -0.0531,  0.0609,  0.0274,\n",
      "         0.0286, -0.0608,  0.3405, -0.3097,  0.3404, -0.2485, -0.3043, -0.2714,\n",
      "         0.3403, -0.2265,  0.2783,  0.3402, -0.3401,  0.3404, -0.3053, -0.2429],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.9609e-01,  3.6787e-01,  1.5818e-03, -5.0325e-02, -1.3951e-01,\n",
      "          1.3307e-01, -1.3997e-01,  2.4246e-01, -1.5420e-01, -2.4687e-01,\n",
      "         -1.9973e-01, -2.0123e-01,  1.4197e-01, -1.1723e-01,  2.6500e-01,\n",
      "          5.1976e-01],\n",
      "        [-2.1614e-01, -7.1932e-02,  2.5721e-02,  4.4534e-01, -1.1065e-03,\n",
      "          1.6676e-01,  1.2831e-01,  1.3609e-01, -5.8786e-02, -7.1149e-02,\n",
      "          1.8199e-01,  9.9746e-03,  1.4974e-01, -4.2164e-01,  1.1167e-01,\n",
      "         -1.5146e-02],\n",
      "        [ 1.6067e-01,  2.3252e-01,  3.1487e-01, -2.8472e-03, -3.7756e-02,\n",
      "         -2.0556e-01, -4.8850e-01,  3.0663e-02,  3.2535e-01, -1.4264e-01,\n",
      "         -3.9334e-01,  3.6075e-01, -1.4457e-01,  9.9327e-02, -1.7082e-01,\n",
      "         -3.4524e-04],\n",
      "        [-1.8262e-01,  2.8253e-02, -1.5742e-04,  2.1104e-01,  1.4702e-01,\n",
      "          9.7979e-02,  1.3082e-01,  1.7416e-01, -2.1880e-01, -1.5666e-01,\n",
      "          3.2569e-01, -1.0101e-01,  2.3595e-01, -3.0798e-01, -1.3148e-02,\n",
      "          1.7885e-01],\n",
      "        [ 2.2033e-01,  3.5861e-01,  4.0623e-01, -7.3776e-02, -1.2338e-01,\n",
      "         -3.1361e-01, -4.1518e-01, -3.1734e-01, -4.6551e-02, -1.0191e-01,\n",
      "         -9.0438e-02,  4.1412e-01, -1.8806e-01,  4.8764e-01, -1.3948e-01,\n",
      "         -2.7940e-01],\n",
      "        [-2.8708e-01, -1.3511e-01, -4.6578e-02,  3.6603e-02, -2.9144e-02,\n",
      "          6.9459e-02, -9.2847e-02,  2.2203e-01, -3.1497e-01, -4.7836e-01,\n",
      "          1.9309e-01,  1.6659e-01,  1.0122e-01,  1.4733e-01, -6.4242e-02,\n",
      "          3.8339e-01],\n",
      "        [-4.6119e-01, -1.3850e-01,  1.2590e-01,  4.5856e-01,  2.4142e-01,\n",
      "          2.3790e-01, -4.2353e-01,  2.5431e-01, -4.3303e-01, -1.7652e-01,\n",
      "          1.5958e-01,  1.3929e-02,  5.9071e-01, -4.3120e-01,  2.8864e-01,\n",
      "          3.8214e-01],\n",
      "        [ 4.0366e-01,  1.9193e-01,  4.1744e-01, -9.8773e-02, -2.9825e-01,\n",
      "         -3.4971e-01, -2.0220e-01, -2.1572e-01,  1.1543e-01, -1.0121e-01,\n",
      "         -4.5813e-01,  2.7304e-01, -3.5102e-01,  1.8138e-01, -3.1412e-01,\n",
      "          1.5637e-01],\n",
      "        [ 2.3650e-01, -2.4109e-01, -3.0652e-01, -3.3621e-01, -2.1974e-01,\n",
      "         -5.7177e-01,  1.3287e-01, -4.8283e-01, -5.4915e-02,  5.4011e-01,\n",
      "          1.5048e-01, -2.0821e-01, -5.7810e-01,  3.9498e-01, -2.1930e-01,\n",
      "          3.7694e-01],\n",
      "        [-4.3877e-01, -1.0784e-03, -1.0537e-01,  7.7747e-02,  2.3760e-01,\n",
      "          3.8584e-01, -2.1681e-01,  1.2974e-01, -3.5694e-01,  1.7122e-01,\n",
      "          3.8464e-01, -2.4788e-01,  2.9736e-01, -5.6421e-01,  2.8399e-01,\n",
      "          1.5719e-01],\n",
      "        [-2.1764e-01, -3.0264e-01,  2.4146e-01, -2.5041e-01,  3.9433e-02,\n",
      "         -3.7790e-01,  1.8650e-02, -3.1030e-01,  3.6629e-01, -2.2622e-01,\n",
      "          2.7929e-01,  3.5369e-01, -2.1878e-01, -1.4360e-01,  4.6591e-01,\n",
      "         -2.1898e-01],\n",
      "        [-1.1532e-01, -2.4566e-01, -3.2094e-01, -4.6170e-01, -2.8029e-01,\n",
      "          4.3911e-01,  5.1568e-01, -1.7510e-01, -2.2008e-01,  3.4733e-01,\n",
      "          1.2575e-01, -4.3845e-01,  5.7441e-01, -2.5760e-01, -2.4463e-01,\n",
      "          1.4881e-01],\n",
      "        [ 1.1538e-01,  8.7955e-02,  1.9700e-01, -1.1285e-01, -2.3580e-01,\n",
      "         -3.3038e-01, -1.8118e-01, -4.7312e-01,  4.8298e-01,  3.5294e-01,\n",
      "         -2.1110e-01,  2.0965e-01, -4.7498e-01,  5.2545e-01, -2.6228e-01,\n",
      "         -1.2004e-01],\n",
      "        [ 1.0134e-01, -9.9789e-02, -2.9481e-01, -1.3559e-01, -2.1249e-01,\n",
      "         -3.4043e-01,  1.0738e-01, -4.6024e-01,  1.5717e-01,  2.4538e-01,\n",
      "         -2.8348e-02, -3.5970e-01, -3.0300e-01,  1.8491e-01, -5.0793e-01,\n",
      "         -1.6084e-01],\n",
      "        [ 4.3642e-02, -9.4828e-02, -1.6693e-01, -3.0043e-01, -3.4511e-02,\n",
      "          1.4927e-01,  1.1550e-01, -4.3430e-01, -5.6358e-01,  3.3463e-01,\n",
      "          8.7036e-02, -1.4451e-01,  3.8971e-02, -1.6053e-01, -2.5683e-02,\n",
      "         -1.9190e-01],\n",
      "        [ 6.8129e-02, -2.0008e-01, -7.2090e-02, -4.2087e-01, -1.5063e-01,\n",
      "         -5.5289e-01, -5.2040e-03, -1.4008e-01,  7.5170e-02,  1.8008e-01,\n",
      "         -9.9586e-02, -7.0274e-02, -2.8830e-01,  2.6904e-02,  2.7507e-02,\n",
      "         -3.2478e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3405,  0.0497,  0.3378, -0.3406,  0.2867, -0.2945, -0.3409,  0.1757,\n",
      "         0.3399, -0.3407,  0.3409, -0.3338,  0.3416,  0.0732, -0.1010,  0.3405],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2530,  0.5050,  0.4055,  ...,  0.3328, -0.1656,  0.3776],\n",
      "        [-0.2274,  0.5348, -0.1252,  ..., -0.5594,  0.2030,  0.5447],\n",
      "        [-0.1818, -0.2515,  0.1176,  ...,  0.0446,  0.0839, -0.5827],\n",
      "        ...,\n",
      "        [ 0.2432,  0.4169, -0.4519,  ...,  0.5416, -0.3185,  0.4408],\n",
      "        [-0.4814,  0.3172, -0.3813,  ..., -0.1246,  0.2652,  0.1277],\n",
      "        [ 0.4585, -0.0032, -0.0738,  ...,  0.5164,  0.0634, -0.3002]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3356,  0.2543, -0.1302,  ..., -0.3328,  0.4318, -0.1685],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3234, -0.3455, -0.2867,  ..., -0.3290, -0.3309, -0.2025],\n",
      "        [ 0.3369,  0.3241, -0.2916,  ...,  0.3502,  0.3156,  0.2245],\n",
      "        [ 0.3373,  0.3636, -0.3540,  ...,  0.3692,  0.3241,  0.1633],\n",
      "        ...,\n",
      "        [-0.3476, -0.3392, -0.1995,  ...,  0.3220, -0.3560,  0.2303],\n",
      "        [-0.2803, -0.3154,  0.3388,  ...,  0.3637, -0.3104,  0.3153],\n",
      "        [-0.3509, -0.3318,  0.3148,  ...,  0.3360, -0.3488,  0.3248]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3153,  0.3423, -0.0870, -0.3154, -0.3319, -0.3491, -0.3520,  0.3495,\n",
      "         0.3495, -0.3709,  0.3591, -0.3315,  0.3691, -0.3531,  0.3186,  0.3506],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6650, 1.3140, 0.6590, 1.3421, 1.2468, 1.2172, 0.6598, 1.2403, 1.3406,\n",
      "        0.6592, 1.3410, 1.3359, 1.3425, 0.6757, 1.3407, 0.7145],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3395, -0.0779,  0.3369, -0.3409,  0.3008,  0.1821, -0.3413,  0.1619,\n",
      "         0.3404, -0.3415,  0.3415, -0.3346,  0.3421, -0.1566, -0.2015,  0.3413],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6636, 1.2728, 0.6584, 1.3413, 0.8549, 0.6626, 0.6598, 0.6735, 1.3377,\n",
      "        0.6586, 1.3409, 1.3359, 1.3419, 0.6714, 1.3391, 0.6903],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1027, -0.1016,  0.3374, -0.3351,  0.3009,  0.2321, -0.3414,  0.3327,\n",
      "         0.3404, -0.3418,  0.3412, -0.3357,  0.3420, -0.1761, -0.2500,  0.3414],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7196, 0.7973, 0.7335, 0.7071, 1.0499, 1.3365, 0.8026, 0.6920, 1.0623,\n",
      "        0.6563, 0.6559, 1.3386, 0.7442, 0.7549, 1.3415, 0.6582],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0628, -0.1793, -0.1960, -0.3403, -0.1421, -0.2749, -0.3396,  0.0289,\n",
      "         0.1233, -0.1846, -0.2635, -0.1604,  0.0052, -0.3363,  0.0762, -0.2703],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in transformer_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4538, -0.2701, -0.4398, -0.1409, -0.4429, -0.3758, -0.2304, -0.1857,\n",
      "         -0.3239, -0.2338, -0.3373, -0.2027, -0.3353, -0.4373, -0.2311, -0.3610,\n",
      "         -0.2635, -0.4163, -0.1515, -0.4030, -0.3558, -0.3782, -0.1805, -0.2047,\n",
      "         -0.1759],\n",
      "        [ 0.3961,  0.4494,  0.1492,  0.2543,  0.2449,  0.2459,  0.1655,  0.5146,\n",
      "          0.2694,  0.3462,  0.4701,  0.4781,  0.1442,  0.3162,  0.2654,  0.4356,\n",
      "          0.4596,  0.1905,  0.2220,  0.4229,  0.5270,  0.2126,  0.1606,  0.3462,\n",
      "          0.5162],\n",
      "        [ 0.2327,  0.3325,  0.5104,  0.4169,  0.3744,  0.2273,  0.5243,  0.3295,\n",
      "          0.3064,  0.3645,  0.3059,  0.2649,  0.2353,  0.3190,  0.4636,  0.5157,\n",
      "          0.1994,  0.3281,  0.2890,  0.1623,  0.2992,  0.4834,  0.3727,  0.4598,\n",
      "          0.3554],\n",
      "        [ 0.1950,  0.1918,  0.5399,  0.4330,  0.2199,  0.4007,  0.4329,  0.3163,\n",
      "          0.3072,  0.2565,  0.3705,  0.3214,  0.3836,  0.3533,  0.5200,  0.3922,\n",
      "          0.3015,  0.4710,  0.3370,  0.1424,  0.2422,  0.2568,  0.4085,  0.2656,\n",
      "          0.4369],\n",
      "        [ 0.2221,  0.2542,  0.3705,  0.3686,  0.3077,  0.3514,  0.3763,  0.1875,\n",
      "          0.4037,  0.3771,  0.3946,  0.2059,  0.3816,  0.1574,  0.4910,  0.3093,\n",
      "          0.3531,  0.4995,  0.2056,  0.4676,  0.4634,  0.2809,  0.3918,  0.2395,\n",
      "          0.2539],\n",
      "        [-0.5229, -0.1495, -0.2271, -0.5074, -0.2915, -0.2182, -0.2942, -0.4477,\n",
      "         -0.4756, -0.4942, -0.3764, -0.1426, -0.1783, -0.1635, -0.4383, -0.4825,\n",
      "         -0.3313, -0.4096, -0.1900, -0.3176, -0.4026, -0.2253, -0.2489, -0.4469,\n",
      "         -0.1484],\n",
      "        [ 0.3793,  0.5398,  0.2162,  0.2604,  0.2622,  0.2967,  0.2735,  0.1547,\n",
      "          0.4364,  0.1405,  0.4957,  0.3791,  0.2268,  0.3916,  0.3847,  0.4046,\n",
      "          0.4435,  0.3887,  0.2626,  0.1950,  0.4286,  0.5093,  0.3497,  0.4788,\n",
      "          0.3963],\n",
      "        [-0.3730, -0.4824, -0.2982, -0.3046, -0.3721, -0.3873, -0.4612, -0.2946,\n",
      "         -0.2689, -0.4006, -0.4420, -0.2185, -0.3761, -0.1627, -0.2301, -0.3684,\n",
      "         -0.4450, -0.3012, -0.2067, -0.4391, -0.2711, -0.3929, -0.4761, -0.4846,\n",
      "         -0.5173],\n",
      "        [-0.2013, -0.3358, -0.4754, -0.4579, -0.4702, -0.3156, -0.4293, -0.3887,\n",
      "         -0.3860, -0.1868, -0.2560, -0.4798, -0.1419, -0.3881, -0.1431, -0.4266,\n",
      "         -0.4208, -0.4763, -0.3263, -0.1526, -0.3021, -0.2871, -0.4005, -0.4596,\n",
      "         -0.3030],\n",
      "        [ 0.4197,  0.2648,  0.3862,  0.3870,  0.2094,  0.2504,  0.1792,  0.2482,\n",
      "          0.2438,  0.4198,  0.2518,  0.3030,  0.3900,  0.2161,  0.2731,  0.1502,\n",
      "          0.1941,  0.1341,  0.3698,  0.2157,  0.1782,  0.3015,  0.2663,  0.2841,\n",
      "          0.3512],\n",
      "        [ 0.3605,  0.4988,  0.4816,  0.4853,  0.2814,  0.3197,  0.1740,  0.5289,\n",
      "          0.3680,  0.3332,  0.3487,  0.2002,  0.5329,  0.5279,  0.3624,  0.5248,\n",
      "          0.4379,  0.2271,  0.2892,  0.4117,  0.3935,  0.4009,  0.2917,  0.3983,\n",
      "          0.3662],\n",
      "        [ 0.5325,  0.4978,  0.1613,  0.4541,  0.2888,  0.4716,  0.2306,  0.2014,\n",
      "          0.5296,  0.3031,  0.3507,  0.1822,  0.4310,  0.3887,  0.1861,  0.5378,\n",
      "          0.3600,  0.2066,  0.3114,  0.2227,  0.5051,  0.3181,  0.2719,  0.2093,\n",
      "          0.2219],\n",
      "        [-0.2253, -0.3857, -0.3915, -0.1575, -0.3586, -0.2852, -0.2362, -0.1732,\n",
      "         -0.3380, -0.5355, -0.3959, -0.4725, -0.4446, -0.2581, -0.1818, -0.2000,\n",
      "         -0.5081, -0.5136, -0.2970, -0.1682, -0.3122, -0.3997, -0.5238, -0.4854,\n",
      "         -0.4221],\n",
      "        [-0.0282, -0.2281, -0.1959, -0.0228, -0.2415, -0.2925, -0.1053,  0.0172,\n",
      "         -0.3030,  0.0093, -0.0530, -0.0552, -0.1537, -0.3544, -0.1445, -0.0386,\n",
      "         -0.2180, -0.0443, -0.1690, -0.3194, -0.1547, -0.0878, -0.0820, -0.2338,\n",
      "         -0.0847],\n",
      "        [-0.2583, -0.4336, -0.3132, -0.2668, -0.3080, -0.3248, -0.3327, -0.1781,\n",
      "         -0.4066, -0.2396, -0.1617, -0.1726, -0.2439, -0.2234, -0.3885, -0.3101,\n",
      "         -0.3056, -0.5327, -0.3822, -0.3451, -0.4667, -0.3918, -0.2013, -0.5024,\n",
      "         -0.3143],\n",
      "        [-0.4800, -0.2534, -0.4605, -0.2263, -0.2180, -0.1750, -0.4854, -0.1704,\n",
      "         -0.3070, -0.3834, -0.4224, -0.1799, -0.5114, -0.1743, -0.2734, -0.2982,\n",
      "         -0.4867, -0.3365, -0.4676, -0.5042, -0.1859, -0.2293, -0.2821, -0.4740,\n",
      "         -0.4826]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3601,  0.3417,  0.5308,  0.4872,  0.2377, -0.3986,  0.2555, -0.2111,\n",
      "        -0.2609,  0.4436,  0.4707,  0.4199, -0.2825, -0.4249, -0.2564, -0.2777],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0055,  0.4580,  0.2055,  0.6826,  0.0710,  0.1878,  0.2455,  0.0325],\n",
      "        [-0.4991, -0.2485, -0.6188, -0.5341, -0.3603, -0.3135, -0.1017, -0.1300],\n",
      "        [-0.1513, -0.1595, -0.6036, -0.1600, -0.5361, -0.0480, -0.4428, -0.6282],\n",
      "        [ 0.4395,  0.2346,  0.3660,  0.0059,  0.1171,  0.6506,  0.5183,  0.2780],\n",
      "        [ 0.3433, -0.0844, -0.1938, -0.1201,  0.1245,  0.0693, -0.0581,  0.1247],\n",
      "        [ 0.0059,  0.4077,  0.5376,  0.0696, -0.2183, -0.1872, -0.1732, -0.1537],\n",
      "        [-0.5137, -0.3095, -0.0557,  0.0116, -0.3994, -0.3045, -0.0364, -0.2888],\n",
      "        [ 0.0118,  0.5353,  0.6341,  0.5125,  0.5426,  0.1294,  0.0797,  0.4473],\n",
      "        [ 0.5991,  0.3965,  0.5113,  0.1549,  0.3589,  0.2654,  0.5217,  0.4303],\n",
      "        [-0.3850,  0.2645, -0.0707, -0.1279, -0.0630,  0.1465, -0.2723, -0.0073],\n",
      "        [ 0.3089,  0.5294,  0.5469,  0.3762,  0.3699,  0.1866,  0.5179,  0.1907],\n",
      "        [-0.3265, -0.4243, -0.1214, -0.4042, -0.1744, -0.6010, -0.5130, -0.5393],\n",
      "        [ 0.4781,  0.2893,  0.2183,  0.3598,  0.3139,  0.1592,  0.6740,  0.0460],\n",
      "        [ 0.4645,  0.1556,  0.5226,  0.2008,  0.2332,  0.1579,  0.3257,  0.2880],\n",
      "        [ 0.5261,  0.4553,  0.6264,  0.3048,  0.2643,  0.6103,  0.1935,  0.6744],\n",
      "        [ 0.2185,  0.4345, -0.0049,  0.0747,  0.1711,  0.5157,  0.0702,  0.4426]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4025,  0.5102,  0.1023, -0.6675,  0.1434,  0.5113, -0.4510, -0.5396,\n",
      "         0.3294, -0.1424,  0.5381,  0.0434,  0.5276,  0.3110, -0.2173,  0.5616],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.4891e-02, -2.1427e-01, -4.6107e-01,  4.6840e-02, -3.7783e-01,\n",
      "         -1.5310e-01, -2.8633e-01,  2.5204e-01,  5.3301e-01, -7.5820e-02,\n",
      "          2.2154e-01, -2.7105e-01,  2.7674e-01,  3.1024e-02,  4.9463e-01,\n",
      "          5.6375e-01],\n",
      "        [-1.4395e-01, -4.9186e-01, -4.9158e-01, -1.6427e-01, -5.2194e-01,\n",
      "         -1.8636e-01, -5.1455e-01,  5.2236e-01,  1.0478e-01, -1.8237e-01,\n",
      "         -6.0172e-02, -5.7538e-01,  2.5342e-01, -6.3022e-02,  3.6365e-01,\n",
      "          5.0010e-01],\n",
      "        [-1.4999e-01, -2.8216e-01, -3.7604e-01, -6.3333e-02, -3.5126e-01,\n",
      "         -4.9931e-01, -1.9737e-01,  3.7035e-01,  2.8507e-01, -1.5568e-01,\n",
      "          9.2463e-02, -4.7721e-01,  5.6951e-01,  1.9962e-02,  2.8220e-01,\n",
      "          5.4938e-01],\n",
      "        [-1.4712e-01, -4.1305e-01, -4.0873e-01, -9.3667e-02, -2.1268e-01,\n",
      "         -4.7192e-01, -3.8698e-01,  1.0255e-01,  2.4334e-01, -1.2128e-01,\n",
      "         -1.0549e-01, -5.1620e-01,  4.3346e-01, -2.8939e-02,  3.6882e-01,\n",
      "          2.5551e-01],\n",
      "        [-1.2207e-02, -3.2773e-01, -1.3929e-01,  6.0225e-02, -2.5088e-01,\n",
      "         -5.4082e-01, -3.9070e-01,  4.7273e-01,  4.7939e-01,  5.4871e-02,\n",
      "          3.3244e-01, -1.6200e-01,  2.0376e-01, -1.5432e-01,  2.7810e-01,\n",
      "          1.9192e-01],\n",
      "        [ 3.5189e-02, -2.4273e-01, -2.7840e-01, -9.1544e-02, -4.7990e-01,\n",
      "         -2.7474e-01, -3.6167e-01,  2.3048e-01,  3.9292e-01,  5.1237e-03,\n",
      "          7.1842e-02, -5.8149e-01,  1.9912e-01, -4.6779e-02,  5.2503e-01,\n",
      "          1.7093e-01],\n",
      "        [ 4.1571e-02,  5.1064e-03, -9.3255e-02,  1.9003e-02, -4.8424e-01,\n",
      "         -2.9499e-01, -3.6496e-01,  3.8112e-01,  3.1448e-01, -2.0638e-01,\n",
      "         -1.1012e-01, -5.7007e-01,  4.4620e-01, -1.5631e-01,  2.8908e-01,\n",
      "          4.1117e-01],\n",
      "        [-1.6969e-01, -1.8477e-01, -5.6510e-01,  1.6335e-02, -5.3475e-01,\n",
      "         -3.7553e-01, -1.4610e-01,  2.0475e-01,  3.2164e-01, -5.2726e-02,\n",
      "         -1.1963e-01, -4.6237e-01,  4.1717e-01,  1.6188e-04,  4.0125e-01,\n",
      "          9.1749e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2603, 0.1958, 0.3043, 0.4650, 0.1984, 0.3615, 0.3021, 0.5630],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4646, -0.3282, -0.4327, -0.2439, -0.2560, -0.0692, -0.5799, -0.1159,\n",
      "          0.4851,  0.3413, -0.3337, -0.0857,  0.1630, -0.3438, -0.1901, -0.2206],\n",
      "        [-0.3756,  0.3871, -0.2875, -0.2893, -0.1603, -0.4342, -0.2023, -0.6145,\n",
      "         -0.5436, -0.0736, -0.0489, -0.3348,  0.2560, -0.0751, -0.1110, -0.5969],\n",
      "        [-0.2925,  0.0911, -0.0856, -0.4420, -0.5444, -0.3304, -0.4248, -0.0465,\n",
      "         -0.6363,  0.1482, -0.5346, -0.5502, -0.0981, -0.3332, -0.5929, -0.1548],\n",
      "        [ 0.0935, -0.0813,  0.1484,  0.0650,  0.4480,  0.3543,  0.4404,  0.3588,\n",
      "          0.1568,  0.0648,  0.2050,  0.5417, -0.0336,  0.3838,  0.5425,  0.4128],\n",
      "        [ 0.0473,  0.1499,  0.1998,  0.0827, -0.4862,  0.5098, -0.4171, -0.5768,\n",
      "         -0.4597,  0.1380, -0.2121, -0.0464, -0.3218, -0.0671, -0.2240, -0.3379],\n",
      "        [-0.5995, -0.3541, -0.5472, -0.5550, -0.4338,  0.0145, -0.5323,  0.2888,\n",
      "         -0.0945,  0.1478, -0.4230, -0.5329,  0.0379, -0.3006,  0.1534, -0.3909],\n",
      "        [ 0.3143, -0.5474,  0.5066,  0.4403, -0.0768,  0.3348, -0.0660,  0.6406,\n",
      "         -0.4493,  0.4754,  0.4076,  0.1225, -0.4341,  0.4816,  0.4238,  0.6104],\n",
      "        [ 0.0715, -0.6259,  0.0495,  0.2390, -0.2362,  0.6136,  0.3799,  0.0691,\n",
      "          0.5350,  0.0214,  0.3418,  0.4711,  0.1812,  0.2556,  0.2845,  0.4953],\n",
      "        [-0.5707,  0.5734, -0.3077, -0.3987,  0.0214, -0.3041, -0.4906, -0.3799,\n",
      "         -0.5260,  0.1109, -0.2219, -0.5306,  0.2713, -0.0447, -0.2051, -0.2116],\n",
      "        [-0.5911,  0.1445, -0.4786,  0.0968, -0.0686, -0.3117, -0.1064, -0.4669,\n",
      "         -0.0720,  0.0633, -0.4121, -0.0921,  0.1234, -0.4638, -0.4787, -0.5034],\n",
      "        [-0.2436,  0.1331,  0.0270,  0.0606,  0.2906, -0.5530,  0.1936, -0.3511,\n",
      "         -0.2372, -0.1767, -0.1589, -0.1266, -0.3479, -0.3660, -0.5161, -0.2672],\n",
      "        [-0.1857,  0.0132, -0.4819, -0.1875, -0.0820, -0.1490, -0.2137,  0.3499,\n",
      "          0.5596, -0.1035, -0.2795, -0.1725,  0.4037, -0.1378,  0.3933, -0.0467],\n",
      "        [-0.6080,  0.2133, -0.0923, -0.0201,  0.4295, -0.1650,  0.3338, -0.3468,\n",
      "         -0.2142, -0.0049, -0.2792, -0.1031, -0.1884, -0.1710, -0.1945, -0.5566],\n",
      "        [ 0.1862, -0.0880, -0.2646, -0.3674, -0.2425,  0.1002,  0.0188,  0.1926,\n",
      "          0.1396, -0.4180, -0.1438,  0.0502,  0.6319,  0.3069,  0.1200,  0.3728],\n",
      "        [-0.1004,  0.1590, -0.4134, -0.1356, -0.0727, -0.3085, -0.0728, -0.4159,\n",
      "          0.3253, -0.1481, -0.2262, -0.5831,  0.5820, -0.2512, -0.4081, -0.1311],\n",
      "        [-0.5943, -0.0660, -0.2574, -0.2243, -0.4202, -0.3009, -0.6221, -0.5979,\n",
      "         -0.3998, -0.2759, -0.4179, -0.2104, -0.0906, -0.4117, -0.3517, -0.5636],\n",
      "        [-0.3474,  0.0609, -0.2883, -0.6207, -0.2366, -0.4317,  0.4106, -0.6440,\n",
      "         -0.0909,  0.4618,  0.6055,  0.2664, -0.2241, -0.3646, -0.0785, -0.2347],\n",
      "        [-0.3998,  0.3043, -0.0819, -0.2796, -0.4619, -0.2954, -0.2344,  0.2358,\n",
      "          0.5601,  0.5400, -0.2835,  0.2416,  0.4293, -0.0507,  0.2360,  0.0939],\n",
      "        [ 0.4149, -0.4837,  0.1085, -0.4760,  0.2479,  0.5984,  0.2384, -0.0181,\n",
      "         -0.4637,  0.2602,  0.4583,  0.0591, -0.3628,  0.5461, -0.4308, -0.3083],\n",
      "        [ 0.1739, -0.1895,  0.3402,  0.1225, -0.4976, -0.5423,  0.3296, -0.3849,\n",
      "          0.4246, -0.0640,  0.4663,  0.1188, -0.2637,  0.1574,  0.0668, -0.2176],\n",
      "        [-0.3099,  0.3639, -0.4030, -0.1627, -0.1581, -0.2317, -0.4623, -0.4771,\n",
      "          0.5515,  0.2726,  0.5377,  0.2066,  0.5314, -0.3602, -0.5236, -0.0964],\n",
      "        [ 0.5422, -0.0956,  0.4788,  0.1249,  0.3102,  0.6409,  0.5682,  0.0537,\n",
      "         -0.2943, -0.3187, -0.5156, -0.3367, -0.2232,  0.3489,  0.2789,  0.0880],\n",
      "        [-0.6292,  0.0844, -0.5005, -0.5200, -0.5082, -0.2496, -0.1363, -0.5909,\n",
      "          0.4757,  0.4967,  0.3913,  0.3634, -0.4863, -0.2038, -0.5035, -0.1595],\n",
      "        [-0.0967,  0.4593, -0.4349, -0.2974,  0.0231, -0.5806,  0.0422, -0.1364,\n",
      "          0.0348,  0.5534,  0.4569,  0.0674, -0.5222, -0.6369, -0.3240, -0.3277],\n",
      "        [-0.4144, -0.2012,  0.4089, -0.0887,  0.1899, -0.4096, -0.5672,  0.0626,\n",
      "          0.1615,  0.3752, -0.1289,  0.3261,  0.4121, -0.2787, -0.1554,  0.4134],\n",
      "        [ 0.1996,  0.1986, -0.5948,  0.5954, -0.4039,  0.2886, -0.1511,  0.5380,\n",
      "          0.5766,  0.0221, -0.0608, -0.5687,  0.3275,  0.1293,  0.6263,  0.3298],\n",
      "        [ 0.2737, -0.1751, -0.5515, -0.3397, -0.1849,  0.3655, -0.0926,  0.5557,\n",
      "          0.2959,  0.3297, -0.0936, -0.4660,  0.5520, -0.2111,  0.4083,  0.1518],\n",
      "        [-0.2513, -0.1771,  0.2793, -0.2426,  0.0855, -0.5492,  0.0328, -0.3516,\n",
      "         -0.4012,  0.1157,  0.0214,  0.5922, -0.2215, -0.4993, -0.3918, -0.0918],\n",
      "        [ 0.0729, -0.1212, -0.2680,  0.4942, -0.5304,  0.3037, -0.5006,  0.4167,\n",
      "          0.0842,  0.0376, -0.4214, -0.5539,  0.4773,  0.2504,  0.5600,  0.5212],\n",
      "        [-0.0976, -0.2104,  0.1038, -0.1183,  0.6086, -0.5674,  0.5266, -0.3233,\n",
      "         -0.0420, -0.1044,  0.2011,  0.2556, -0.2540,  0.3182,  0.2466, -0.0591],\n",
      "        [-0.2300, -0.0659,  0.1148, -0.3835,  0.5078, -0.3632, -0.1818,  0.3009,\n",
      "         -0.0711,  0.4388, -0.0749,  0.1685,  0.0140, -0.1389, -0.2771, -0.4344],\n",
      "        [ 0.6001,  0.1473, -0.1996,  0.6019, -0.2877,  0.3933, -0.5865,  0.1486,\n",
      "          0.1092,  0.0876, -0.2378, -0.2533,  0.0056,  0.6103,  0.6340,  0.3327],\n",
      "        [-0.1539,  0.0581, -0.1262, -0.2867, -0.2111,  0.5939, -0.2063, -0.2937,\n",
      "         -0.2886, -0.0075, -0.0436, -0.1672,  0.1415, -0.4772, -0.0854, -0.1883],\n",
      "        [-0.2464,  0.1555, -0.2110, -0.2869,  0.1337,  0.2659, -0.5101, -0.6055,\n",
      "         -0.2700, -0.3370, -0.3691, -0.1630,  0.0929, -0.5601, -0.3495, -0.1695],\n",
      "        [ 0.1056, -0.1107,  0.0802,  0.1769, -0.4856,  0.1284,  0.2013,  0.5747,\n",
      "          0.1790, -0.0460,  0.4592,  0.1900,  0.0764,  0.0627,  0.5461,  0.2988],\n",
      "        [ 0.4720, -0.6113,  0.4804,  0.3442, -0.4347, -0.1433,  0.4880,  0.5517,\n",
      "          0.1917,  0.4075,  0.5797,  0.0622, -0.3751,  0.0626,  0.3947,  0.1007],\n",
      "        [-0.3421,  0.1119, -0.2883, -0.2463,  0.5627,  0.5672, -0.2060, -0.1877,\n",
      "         -0.3711, -0.3162, -0.5993, -0.6018,  0.5248, -0.1810, -0.5203, -0.1252],\n",
      "        [-0.1584,  0.5561, -0.2700, -0.1281,  0.4830, -0.1658, -0.1405, -0.1984,\n",
      "         -0.1867, -0.0118, -0.6252, -0.5588,  0.2187, -0.2245, -0.1962, -0.5279],\n",
      "        [-0.1786,  0.0570, -0.1749, -0.6198,  0.4239,  0.1173, -0.0369, -0.4506,\n",
      "         -0.4408, -0.1498, -0.4612, -0.5774,  0.2779, -0.5920, -0.4665, -0.5798],\n",
      "        [ 0.5180, -0.6042,  0.4778,  0.3833, -0.3208, -0.0459,  0.0816,  0.0776,\n",
      "          0.3573,  0.5510,  0.5589,  0.4770, -0.1676,  0.0369,  0.1868,  0.1859],\n",
      "        [ 0.2521, -0.5939, -0.1956, -0.2898, -0.3181,  0.3473, -0.1911,  0.2783,\n",
      "          0.4264,  0.5768, -0.0507, -0.1459,  0.5951, -0.0292,  0.0478,  0.1264],\n",
      "        [ 0.1107, -0.1233,  0.5046,  0.4547, -0.2862, -0.2170,  0.3387,  0.3819,\n",
      "          0.2736,  0.3119,  0.6131,  0.4055, -0.1845,  0.1910,  0.0733,  0.6398],\n",
      "        [ 0.4797, -0.3870,  0.1847,  0.3310, -0.3324, -0.0935,  0.2653,  0.3464,\n",
      "          0.2897,  0.5974,  0.3269,  0.5797,  0.0603,  0.1286,  0.2173,  0.5144],\n",
      "        [ 0.2171, -0.6023,  0.5912,  0.3736, -0.3047, -0.5555,  0.3780,  0.1638,\n",
      "          0.0632,  0.2598,  0.0750,  0.3021, -0.5205,  0.2121,  0.3843,  0.1630],\n",
      "        [ 0.4489, -0.1684,  0.4580,  0.0496, -0.0979, -0.6364,  0.2665,  0.2883,\n",
      "          0.5044,  0.2403,  0.1592,  0.3322, -0.4187,  0.3833,  0.2910,  0.2956],\n",
      "        [ 0.1457, -0.1050,  0.4725,  0.3505, -0.4337, -0.4201,  0.3752,  0.2715,\n",
      "          0.3118,  0.4247,  0.3343,  0.6390, -0.4407,  0.6384,  0.5213,  0.3971],\n",
      "        [-0.0777,  0.1570, -0.3448, -0.4711,  0.0364,  0.6257, -0.2565, -0.3623,\n",
      "         -0.3161, -0.4694, -0.3679, -0.1942,  0.3804, -0.5527, -0.0736, -0.4328],\n",
      "        [-0.5174,  0.5749, -0.3939, -0.0898,  0.1322,  0.4635, -0.1457, -0.2205,\n",
      "         -0.0575, -0.1209, -0.4933, -0.2663,  0.2886, -0.0894, -0.3479, -0.1790]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3399, -0.3403, -0.3405,  0.3407, -0.3403, -0.3376,  0.3396,  0.3402,\n",
      "        -0.3399, -0.3407, -0.3411, -0.3404, -0.3404,  0.2377, -0.3401, -0.3405,\n",
      "         0.0145,  0.0059, -0.0143,  0.0161,  0.0217, -0.0221,  0.0284, -0.0040,\n",
      "         0.0063,  0.0051,  0.0018, -0.0067, -0.0067,  0.0098,  0.0099, -0.0074,\n",
      "        -0.3408, -0.3400,  0.3406,  0.3402, -0.3405, -0.3410, -0.3404,  0.3409,\n",
      "        -0.2702,  0.3409,  0.3405,  0.3408,  0.3408,  0.3403, -0.3412, -0.3407],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4410,  0.2513,  0.4148,  0.2046, -0.2883,  0.3790,  0.4022,  0.3599,\n",
      "         -0.5829, -0.2171, -0.4748,  0.4933,  0.4854, -0.4196,  0.0524, -0.1439],\n",
      "        [ 0.4559, -0.2783, -0.4223,  0.5815, -0.4379, -0.1605, -0.2847, -0.0338,\n",
      "          0.1000,  0.0592, -0.2044,  0.0919,  0.2640,  0.4409, -0.0218, -0.3584],\n",
      "        [ 0.5153,  0.3804, -0.4219, -0.4853,  0.3361, -0.4322, -0.3976, -0.4673,\n",
      "          0.5721,  0.0845, -0.4642, -0.2642, -0.2630,  0.2189,  0.4746,  0.1493],\n",
      "        [ 0.3047, -0.4553, -0.1549, -0.4476,  0.0972, -0.4430, -0.1285, -0.2023,\n",
      "          0.4193,  0.5059,  0.2360, -0.2092, -0.4068,  0.5670,  0.0655,  0.2773],\n",
      "        [ 0.4751, -0.5466, -0.3584, -0.1483,  0.1597, -0.3739, -0.4220, -0.2116,\n",
      "          0.4339,  0.4031,  0.4829, -0.5591, -0.1692,  0.2822, -0.3934,  0.1773],\n",
      "        [-0.2514,  0.2616,  0.5873,  0.5853,  0.3004,  0.3350,  0.3349,  0.1362,\n",
      "         -0.3593, -0.3667, -0.2376,  0.1221,  0.1833, -0.4884,  0.2393, -0.1526],\n",
      "        [ 0.4041, -0.2908, -0.3390, -0.2007,  0.2466, -0.3635, -0.5388, -0.5346,\n",
      "          0.2969,  0.5752,  0.0497, -0.4044, -0.5468,  0.4978,  0.2367,  0.1473],\n",
      "        [-0.4996,  0.1672,  0.5433,  0.3702, -0.4363,  0.0931,  0.3559,  0.3009,\n",
      "         -0.1720, -0.3893, -0.3007,  0.5155,  0.1273, -0.2662, -0.2065, -0.2855],\n",
      "        [-0.1004,  0.4467,  0.1674,  0.3115,  0.3132,  0.4229,  0.3543,  0.1428,\n",
      "         -0.1213, -0.5297, -0.5580,  0.1951,  0.1353, -0.2038,  0.3972, -0.5207],\n",
      "        [ 0.2395, -0.1458, -0.1778, -0.1019, -0.3913, -0.1654, -0.1438, -0.5317,\n",
      "          0.2281,  0.4985,  0.4794, -0.1147,  0.2100,  0.1827, -0.4215,  0.2546],\n",
      "        [ 0.1851, -0.3827, -0.1881, -0.3211, -0.1672, -0.4229, -0.2409, -0.1027,\n",
      "          0.1868,  0.5022,  0.1134, -0.3980, -0.2152,  0.5139, -0.5550,  0.1972],\n",
      "        [ 0.4659, -0.3595, -0.2419, -0.4762,  0.5048, -0.4892, -0.3524, -0.2273,\n",
      "          0.4632,  0.3490, -0.3775, -0.2485, -0.4202,  0.1887,  0.2638,  0.3473],\n",
      "        [-0.2009,  0.4710,  0.1027,  0.3628, -0.2958,  0.1946,  0.4039,  0.4635,\n",
      "         -0.2212, -0.2103,  0.0416,  0.1209,  0.1944, -0.4309, -0.3981, -0.2029],\n",
      "        [ 0.2433, -0.1325, -0.2054, -0.2041,  0.4089, -0.4207, -0.4198, -0.1365,\n",
      "          0.3424,  0.5366, -0.0341, -0.2366, -0.2038,  0.1882,  0.1079,  0.5771],\n",
      "        [-0.2814,  0.3692,  0.4666,  0.3392,  0.2703,  0.3808,  0.5175,  0.1452,\n",
      "         -0.1511, -0.5892, -0.2950,  0.1240,  0.1733, -0.4946,  0.4705, -0.2767],\n",
      "        [-0.2263, -0.2742,  0.3997,  0.2567, -0.2571,  0.4381,  0.5111,  0.5159,\n",
      "         -0.4862, -0.1890,  0.1162,  0.1240,  0.4055, -0.5743, -0.4499, -0.4719]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3406,  0.2183,  0.3399,  0.3404,  0.3405, -0.3411,  0.3406, -0.3398,\n",
      "        -0.3394,  0.3411,  0.3401,  0.3403, -0.3406,  0.3404, -0.3408, -0.3388],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5526, -0.0842, -0.5462,  ...,  0.5359, -0.2175,  0.5362],\n",
      "        [ 0.1596, -0.1646,  0.5688,  ..., -0.2127,  0.3772,  0.5706],\n",
      "        [ 0.3450,  0.5222, -0.1076,  ..., -0.4782,  0.1829,  0.5553],\n",
      "        ...,\n",
      "        [ 0.1361,  0.4850, -0.4093,  ...,  0.5886,  0.1457,  0.3755],\n",
      "        [ 0.5479, -0.2541,  0.2967,  ..., -0.0939,  0.2292, -0.2341],\n",
      "        [-0.1682, -0.0508, -0.0756,  ...,  0.1158, -0.0839,  0.0290]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5185,  0.2902,  0.2383,  ...,  0.2145, -0.2393, -0.4554],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3431, -0.3716, -0.3131,  ..., -0.3382, -0.3595, -0.3241],\n",
      "        [-0.3330,  0.3242, -0.3446,  ..., -0.3083,  0.3555,  0.3630],\n",
      "        [-0.2698,  0.3648,  0.0898,  ..., -0.2979,  0.3364,  0.3229],\n",
      "        ...,\n",
      "        [-0.3617, -0.3128, -0.1537,  ..., -0.3435,  0.3579,  0.3343],\n",
      "        [-0.3608, -0.3352, -0.3284,  ..., -0.3509, -0.3155, -0.3195],\n",
      "        [-0.3472, -0.3385,  0.3471,  ...,  0.2589, -0.3503,  0.3180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3697,  0.3095,  0.3253,  0.3680,  0.3673, -0.3436,  0.3615,  0.3488,\n",
      "        -0.3660,  0.3576,  0.3693,  0.3104, -0.3673, -0.2656, -0.3372, -0.3174],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6600, 1.3408, 0.7065, 0.6595, 0.6598, 0.6601, 1.3406, 0.6590, 0.7383,\n",
      "        0.6588, 1.3381, 1.3404, 1.3409, 1.3405, 1.3419, 0.6597],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3406,  0.2901,  0.3399,  0.3404,  0.3405, -0.3411,  0.3405, -0.3396,\n",
      "        -0.3364,  0.3411,  0.3401,  0.3403, -0.3406,  0.3406, -0.3408, -0.2785],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6578, 1.1355, 0.7134, 0.6578, 0.6692, 0.7842, 0.8219, 0.6572, 0.6565,\n",
      "        0.7118, 0.7964, 0.9536, 0.6801, 0.8678, 0.7379, 0.6745],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1210,  0.0983,  0.2437,  0.2677,  0.0152,  0.1962,  0.1306,  0.1566,\n",
      "         0.3395,  0.0428, -0.0504,  0.3353, -0.0448,  0.3023,  0.2233,  0.3409],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0989,  0.3583,  0.1686, -0.0036, -0.2991, -0.0244, -0.4259,  0.0075,\n",
      "          0.0083,  0.6173,  0.4592,  0.1109,  0.5706,  0.4850, -0.2319,  0.1839],\n",
      "        [-0.3551, -0.4712,  0.1791,  0.4153,  0.2371, -0.6300,  0.2784,  0.3722,\n",
      "          0.5697, -0.3535,  0.0550, -0.2520, -0.0174, -0.2401,  0.5823, -0.2661],\n",
      "        [ 0.0348,  0.5340, -0.1074, -0.5532, -0.0366,  0.3862, -0.3988, -0.0766,\n",
      "          0.1811,  0.2098,  0.3660, -0.1933,  0.1649,  0.3152, -0.5600,  0.3773],\n",
      "        [-0.0109, -0.4107,  0.0369, -0.0193,  0.0212, -0.1899,  0.0816,  0.5737,\n",
      "          0.4285,  0.0878, -0.0228, -0.2638,  0.2310,  0.3247, -0.0155,  0.0541],\n",
      "        [-0.3371, -0.0119, -0.1971,  0.5936,  0.3124, -0.3953,  0.5150,  0.2910,\n",
      "          0.3614, -0.3386, -0.3020,  0.0611, -0.0182, -0.1831,  0.3672, -0.2531],\n",
      "        [-0.0048,  0.1707, -0.2930, -0.1407,  0.0248,  0.3322, -0.4059, -0.1804,\n",
      "         -0.1255,  0.4852,  0.1572, -0.0591,  0.1774,  0.0028,  0.0223,  0.5314],\n",
      "        [ 0.3464,  0.1839, -0.2140, -0.5104,  0.1656,  0.4436, -0.3396, -0.3137,\n",
      "         -0.3842,  0.1427, -0.2464,  0.1456,  0.0571, -0.0086, -0.5027,  0.6416],\n",
      "        [ 0.1319, -0.0346,  0.3846,  0.4506,  0.0949,  0.3733,  0.3752, -0.3197,\n",
      "         -0.4248, -0.0258,  0.0199,  0.2001, -0.3517, -0.1333, -0.1446, -0.3170],\n",
      "        [ 0.4166, -0.2448,  0.6278, -0.1685,  0.3511,  0.1651, -0.1240, -0.0832,\n",
      "         -0.5893, -0.2242, -0.1856,  0.2970, -0.2304, -0.2719, -0.3210, -0.1366],\n",
      "        [ 0.2029,  0.0371,  0.1041, -0.1212,  0.2922, -0.0195, -0.1635,  0.3512,\n",
      "         -0.3794, -0.6330, -0.4608,  0.4081, -0.2258, -0.0152, -0.0550, -0.0745],\n",
      "        [ 0.6130,  0.1424,  0.3831,  0.2234,  0.2818, -0.3449,  0.1052, -0.0176,\n",
      "         -0.1998, -0.2222, -0.5302,  0.3971, -0.5744,  0.0352, -0.3314, -0.3901],\n",
      "        [-0.1021, -0.1596, -0.0771, -0.0622, -0.2205, -0.2141,  0.0963, -0.1038,\n",
      "          0.4684,  0.4934,  0.4131, -0.5370,  0.1724, -0.2216,  0.4519, -0.1075],\n",
      "        [ 0.3471, -0.1247,  0.5209, -0.0761,  0.1549,  0.0679, -0.1265,  0.0039,\n",
      "         -0.5520, -0.1278, -0.6266,  0.3233,  0.0689, -0.0269, -0.2825,  0.1911],\n",
      "        [-0.3412, -0.1813, -0.5994, -0.4016, -0.4108,  0.1544,  0.1286,  0.0153,\n",
      "          0.4922,  0.1201,  0.3420, -0.3052,  0.4760,  0.4615,  0.4558,  0.2488],\n",
      "        [-0.1890,  0.0226, -0.6300, -0.1125, -0.5948,  0.0772, -0.3295, -0.2478,\n",
      "          0.1616,  0.4915,  0.1295, -0.2293,  0.1963,  0.2103,  0.6223,  0.1973],\n",
      "        [-0.3312,  0.2663, -0.1590, -0.1702,  0.1283,  0.5788, -0.3312, -0.2511,\n",
      "          0.4051,  0.3594,  0.2088, -0.3176,  0.3369,  0.1806, -0.3095,  0.3380],\n",
      "        [ 0.0128, -0.1040, -0.3346,  0.1907, -0.1231, -0.3846, -0.0940, -0.0059,\n",
      "          0.4891,  0.1122,  0.3753, -0.2865,  0.2038,  0.5360,  0.3344,  0.1354],\n",
      "        [ 0.3621,  0.1158,  0.4914, -0.5402,  0.2939,  0.6216, -0.1074, -0.0102,\n",
      "         -0.3862,  0.1444, -0.5936,  0.0720, -0.1172,  0.3128, -0.5384,  0.6360],\n",
      "        [-0.0845,  0.0676,  0.0955,  0.5166,  0.2098, -0.4743,  0.1887,  0.1809,\n",
      "          0.3231, -0.0380,  0.0770, -0.3835,  0.1144,  0.2076,  0.2225, -0.3864],\n",
      "        [-0.4223,  0.2583, -0.1083, -0.0736,  0.4133,  0.1165,  0.1999, -0.3335,\n",
      "         -0.0467, -0.4687,  0.0805, -0.0056, -0.4832, -0.2805, -0.3988, -0.2281],\n",
      "        [ 0.3601,  0.1972,  0.4160, -0.1372,  0.1247,  0.2462, -0.6298, -0.1148,\n",
      "         -0.3870,  0.2201, -0.5564,  0.4936, -0.1524,  0.1401, -0.1211,  0.3600],\n",
      "        [-0.5916, -0.0475, -0.2712,  0.1783, -0.5197, -0.0415,  0.4033,  0.3582,\n",
      "          0.5375, -0.1663,  0.4486, -0.2790,  0.1384, -0.1977,  0.0904, -0.4331],\n",
      "        [-0.6114, -0.0941, -0.1596,  0.3989, -0.3896, -0.6242,  0.3979,  0.5289,\n",
      "          0.5358,  0.1904,  0.2685, -0.4310,  0.0803,  0.0618,  0.6033, -0.2475],\n",
      "        [ 0.1953, -0.0256,  0.2029, -0.0569, -0.3153,  0.0617, -0.3785, -0.3004,\n",
      "          0.0132, -0.1157, -0.1595,  0.2266,  0.1833, -0.0320, -0.2268,  0.0661],\n",
      "        [-0.5715, -0.0662, -0.6432,  0.2675, -0.3208, -0.0566,  0.5246,  0.5176,\n",
      "          0.6211,  0.3254,  0.2592, -0.2537, -0.1559, -0.1800,  0.4538, -0.3661],\n",
      "        [-0.5184, -0.2677, -0.2250,  0.4295, -0.4107, -0.5433,  0.0327,  0.4958,\n",
      "          0.2462,  0.4976,  0.3655, -0.2051,  0.1355, -0.1749,  0.4160, -0.3800],\n",
      "        [-0.1578, -0.5322, -0.3646,  0.4159, -0.4831, -0.1384,  0.0833,  0.2054,\n",
      "          0.5792,  0.5198,  0.4323, -0.2913,  0.4530, -0.0172,  0.4636, -0.4007],\n",
      "        [ 0.0911,  0.2123,  0.0458, -0.6348,  0.3063,  0.5967, -0.3132, -0.1517,\n",
      "         -0.5674, -0.1248, -0.5125,  0.5633,  0.5056,  0.5732, -0.4893,  0.4938],\n",
      "        [-0.5219, -0.4889, -0.6231,  0.1590, -0.1314, -0.4664,  0.3998,  0.3661,\n",
      "          0.2877, -0.0313,  0.0912, -0.3160, -0.5458, -0.3381,  0.6008, -0.5137],\n",
      "        [ 0.0767,  0.0650,  0.3875, -0.5739,  0.5919,  0.4356, -0.6282, -0.3011,\n",
      "         -0.5399, -0.1046, -0.3031,  0.2128, -0.4376, -0.4084, -0.5556,  0.0607],\n",
      "        [-0.3200,  0.4080,  0.4695, -0.3949, -0.0346,  0.4128,  0.3699, -0.0064,\n",
      "         -0.1606, -0.5962, -0.5190, -0.0408, -0.5844, -0.4407, -0.3115,  0.0699],\n",
      "        [ 0.0596,  0.1612,  0.6324, -0.0523,  0.6028,  0.0491, -0.3010, -0.1028,\n",
      "         -0.6401, -0.1545, -0.1762,  0.3428, -0.0012, -0.2948, -0.4083,  0.5381],\n",
      "        [ 0.5536, -0.0718,  0.3141,  0.5954,  0.3054, -0.0846, -0.0154,  0.4180,\n",
      "         -0.2001, -0.3444, -0.4980,  0.0426, -0.0928, -0.1497, -0.3573, -0.2045],\n",
      "        [-0.3394,  0.4630, -0.3516, -0.2957, -0.1088, -0.1150,  0.6230, -0.0379,\n",
      "          0.0818,  0.3623,  0.4309, -0.2396, -0.1119, -0.1690,  0.4872, -0.5260],\n",
      "        [ 0.1856,  0.3118,  0.3821,  0.6223,  0.6313,  0.4806, -0.1969,  0.4838,\n",
      "         -0.3353, -0.1594, -0.0771,  0.2680, -0.1966,  0.4331, -0.2675,  0.1655],\n",
      "        [ 0.1918, -0.3937,  0.0632,  0.1015,  0.1497,  0.0664, -0.4068,  0.4597,\n",
      "         -0.6035, -0.5392, -0.6341,  0.6190,  0.2127,  0.0968, -0.2069,  0.5184],\n",
      "        [-0.0696,  0.0703, -0.2500, -0.2523, -0.2671,  0.3151,  0.6123, -0.5100,\n",
      "          0.0872, -0.2440,  0.4127, -0.3183,  0.2128, -0.5519,  0.3642, -0.0903],\n",
      "        [-0.2983, -0.2088, -0.3113, -0.3922, -0.2807, -0.5584,  0.0383, -0.4689,\n",
      "          0.1527, -0.2482,  0.0522, -0.4786, -0.0846, -0.4948,  0.3077, -0.4757],\n",
      "        [ 0.2499,  0.0708, -0.2802,  0.1350,  0.3125,  0.0047, -0.5841,  0.4005,\n",
      "          0.2618, -0.1484, -0.4849,  0.1121, -0.1035,  0.0086, -0.1698, -0.0115],\n",
      "        [ 0.4130,  0.1594,  0.1566, -0.2029,  0.1857,  0.3665, -0.0568, -0.4838,\n",
      "         -0.0620, -0.3076,  0.1580,  0.3801,  0.0655, -0.3747,  0.0456,  0.3169],\n",
      "        [ 0.4485, -0.0513,  0.2373,  0.1010,  0.0578,  0.3578, -0.5025,  0.6045,\n",
      "         -0.4762, -0.6309, -0.4474,  0.1574, -0.0642,  0.3208, -0.0987,  0.6279],\n",
      "        [-0.3069,  0.0178, -0.2605,  0.0632, -0.3425, -0.0923,  0.0888,  0.3599,\n",
      "          0.1265,  0.1566,  0.2421, -0.0783, -0.1172,  0.0411,  0.1373, -0.3337],\n",
      "        [ 0.1683, -0.1999, -0.0617,  0.2274,  0.3491,  0.0466, -0.2228,  0.3284,\n",
      "         -0.2070, -0.1615, -0.3603, -0.1701,  0.2071,  0.1015,  0.2293,  0.3647],\n",
      "        [-0.0298,  0.4906,  0.0104, -0.3371, -0.1707,  0.4760, -0.0305, -0.2125,\n",
      "         -0.5417, -0.1354,  0.2145,  0.0219,  0.0649,  0.1975, -0.0156,  0.2176],\n",
      "        [ 0.3510,  0.5065,  0.5209, -0.2178,  0.3974,  0.3306, -0.4006, -0.3432,\n",
      "         -0.0891, -0.0436, -0.4341,  0.1147,  0.0648, -0.4457, -0.0511,  0.3051],\n",
      "        [-0.3429,  0.3459, -0.2427, -0.3143, -0.1848, -0.4170,  0.1781, -0.3860,\n",
      "          0.4335,  0.1023,  0.0593, -0.5513,  0.1695, -0.4918,  0.4930, -0.0587],\n",
      "        [-0.1195, -0.1762, -0.4118, -0.0495, -0.1396, -0.1100,  0.2177,  0.5262,\n",
      "          0.1554,  0.2302,  0.5665, -0.2153,  0.0111, -0.4220,  0.1762, -0.3746],\n",
      "        [ 0.5256,  0.0507, -0.0179,  0.1137,  0.6057, -0.1683, -0.2846,  0.2660,\n",
      "          0.3443,  0.3682, -0.5508,  0.4011,  0.4413,  0.3446, -0.0967,  0.5218]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3432, -0.3399,  0.3160, -0.0995, -0.3258,  0.2685,  0.3282, -0.3166,\n",
      "        -0.3435, -0.3437, -0.2838,  0.3431, -0.3432,  0.3388,  0.3442,  0.2097,\n",
      "         0.0045,  0.1406, -0.0166, -0.1451,  0.1212, -0.0780, -0.1495,  0.0489,\n",
      "        -0.1486, -0.1265, -0.1504,  0.1692, -0.1195,  0.1525,  0.1560,  0.0645,\n",
      "        -0.3231,  0.3402, -0.3415, -0.3410,  0.3399,  0.3400, -0.2894,  0.1576,\n",
      "        -0.3407, -0.2498, -0.3400,  0.3145,  0.2449,  0.3415,  0.3403, -0.3404],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1362,  0.0559, -0.3783, -0.1420, -0.1518,  0.2643, -0.5245, -0.2098,\n",
      "         -0.3512, -0.1154, -0.5575, -0.5180,  0.4147,  0.3383,  0.5123,  0.2774],\n",
      "        [ 0.0889,  0.1291, -0.0859,  0.0888,  0.1133,  0.2912, -0.1955, -0.4201,\n",
      "          0.1540,  0.2298,  0.0309,  0.2284,  0.1060, -0.0559, -0.1389,  0.2314],\n",
      "        [-0.0939,  0.4856,  0.1246, -0.2877,  0.4193, -0.1429,  0.1154,  0.3133,\n",
      "          0.2919, -0.0121,  0.1872,  0.1465,  0.0521, -0.1525, -0.1060, -0.3839],\n",
      "        [ 0.0554,  0.0059, -0.3420, -0.0885,  0.0628,  0.2647, -0.0833,  0.2548,\n",
      "         -0.1338, -0.2243, -0.3161, -0.0887,  0.1912,  0.1264,  0.4700,  0.0801],\n",
      "        [-0.4697,  0.2817,  0.1077, -0.2874,  0.5143,  0.2199,  0.2024,  0.5015,\n",
      "          0.4179,  0.2535,  0.2011,  0.2186,  0.2245,  0.2356, -0.3144, -0.3657],\n",
      "        [-0.2941,  0.1551, -0.5371, -0.5086,  0.3948,  0.2424, -0.1803, -0.4805,\n",
      "         -0.2370,  0.3391, -0.2369, -0.1752,  0.4782,  0.1709,  0.4893, -0.2501],\n",
      "        [ 0.4909,  0.0177, -0.1859, -0.1014,  0.0196,  0.5285, -0.1398, -0.3712,\n",
      "         -0.2745, -0.5003, -0.2570, -0.5258,  0.2853,  0.3095,  0.1736,  0.4963],\n",
      "        [-0.0696, -0.2863,  0.4271,  0.0776, -0.3004, -0.1019,  0.3756,  0.2030,\n",
      "         -0.1163, -0.2788,  0.2645,  0.1801, -0.2804, -0.0991, -0.1569, -0.3426],\n",
      "        [-0.1397, -0.2418,  0.1351,  0.2905, -0.1944, -0.2295,  0.2607,  0.1595,\n",
      "          0.1524, -0.1040,  0.1365,  0.1311, -0.1958, -0.2274, -0.1606, -0.3815],\n",
      "        [ 0.2841, -0.4066, -0.4178,  0.2512, -0.3360,  0.0912, -0.5255, -0.5572,\n",
      "         -0.4522,  0.0757, -0.2356, -0.5088,  0.5868,  0.2909,  0.3323,  0.3078],\n",
      "        [-0.4595, -0.0177,  0.2638, -0.3327, -0.0021, -0.0707,  0.0931,  0.1925,\n",
      "          0.2521,  0.2758,  0.4385, -0.1044, -0.2403, -0.4316, -0.5348, -0.3061],\n",
      "        [-0.5605,  0.0183,  0.3832,  0.0896,  0.1126,  0.2679,  0.1108,  0.4243,\n",
      "          0.1815,  0.4526,  0.2134, -0.1295, -0.3673, -0.4080, -0.2527, -0.2497],\n",
      "        [-0.4095,  0.3097,  0.0897, -0.2388,  0.4214,  0.0049,  0.2131,  0.4772,\n",
      "          0.3158,  0.2531,  0.0556,  0.1476, -0.2601, -0.0693, -0.1099, -0.3812],\n",
      "        [ 0.0832,  0.2032,  0.5440,  0.2350, -0.0333, -0.2469,  0.2298,  0.2327,\n",
      "          0.2737, -0.3972, -0.0147,  0.2457, -0.2179, -0.4938, -0.1061, -0.0903],\n",
      "        [ 0.5161, -0.2650, -0.3126,  0.4304, -0.3316, -0.1071, -0.1783, -0.1990,\n",
      "         -0.0155,  0.0398,  0.3330, -0.1371, -0.1921, -0.5533,  0.3898,  0.3548],\n",
      "        [-0.1613,  0.0382, -0.3361, -0.0086, -0.0027,  0.3485,  0.2207,  0.0317,\n",
      "          0.2375,  0.1186, -0.1807, -0.0654, -0.3482,  0.0220, -0.3481, -0.2455]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3397,  0.3354,  0.3414, -0.3410,  0.3422,  0.2161, -0.3410, -0.1885,\n",
      "         0.3386, -0.3408,  0.3409,  0.3394,  0.3416,  0.0788, -0.2899,  0.3409],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.9580e-01,  3.5915e-01, -5.8660e-02, -1.5414e-01, -6.7091e-02,\n",
      "          4.1606e-01,  5.6947e-02, -3.2193e-01, -3.3426e-01, -1.7062e-01,\n",
      "          6.1457e-01,  7.4296e-02,  5.4068e-01, -4.3390e-01,  2.4239e-01,\n",
      "          2.9405e-01],\n",
      "        [-2.0511e-01,  5.6029e-02, -6.0889e-01, -2.3176e-01, -1.6425e-01,\n",
      "         -2.6616e-01,  4.5815e-01, -3.1487e-01,  3.3502e-01,  5.0708e-01,\n",
      "          4.4518e-01, -4.2889e-01,  5.0651e-01, -3.7278e-02,  1.0615e-01,\n",
      "         -7.1221e-02],\n",
      "        [ 1.0131e-01,  4.7507e-01, -1.2741e-01, -1.3267e-01,  3.4608e-01,\n",
      "          3.0023e-01, -3.9540e-02, -1.7474e-01, -6.8962e-02,  3.9031e-01,\n",
      "         -1.9682e-01,  1.9719e-02,  3.6056e-02,  4.5063e-01, -6.1010e-01,\n",
      "          6.4259e-01],\n",
      "        [-2.1000e-01, -1.7642e-01,  3.3515e-02, -2.8057e-01, -4.7247e-01,\n",
      "         -5.6789e-01,  3.8868e-01, -3.4065e-02,  1.0346e-01, -7.0195e-02,\n",
      "          5.5142e-01, -1.6451e-02,  2.5711e-01, -7.1848e-02,  1.4386e-01,\n",
      "         -4.0122e-01],\n",
      "        [ 1.7309e-01,  2.5399e-01,  5.6272e-01,  2.4932e-02,  4.1392e-01,\n",
      "          3.7145e-02, -6.8293e-02,  8.1816e-02, -1.6750e-01, -4.6565e-01,\n",
      "         -2.5084e-01, -3.4612e-02,  4.9061e-02,  4.2188e-01, -3.4800e-01,\n",
      "          4.0114e-01],\n",
      "        [-5.1492e-02, -3.4509e-01, -7.1017e-02,  3.1401e-01, -1.4527e-01,\n",
      "         -2.9107e-01,  2.6658e-01,  1.7081e-01,  1.8859e-05,  4.9691e-02,\n",
      "          2.2277e-01, -1.1945e-01, -5.3238e-02,  3.3548e-01,  3.2319e-01,\n",
      "         -2.8111e-01],\n",
      "        [-5.5041e-01, -8.1063e-02,  7.4617e-02,  3.9036e-01,  6.7008e-02,\n",
      "         -1.9914e-01,  1.5970e-01,  1.1690e-01,  2.3782e-01, -4.2484e-02,\n",
      "          8.0533e-02, -2.9948e-03, -2.0041e-01,  7.4850e-02,  3.1875e-01,\n",
      "         -4.2981e-01],\n",
      "        [-5.7034e-01, -1.4533e-01, -3.2918e-01,  3.8291e-01, -5.0036e-01,\n",
      "         -1.5033e-01,  1.8920e-01, -2.8369e-01,  1.9404e-01,  2.3293e-01,\n",
      "          6.1776e-01, -5.8819e-01,  3.3352e-02, -4.1431e-01,  3.0834e-01,\n",
      "         -5.1803e-01],\n",
      "        [-5.7333e-01, -3.0431e-01,  3.1126e-01,  2.0875e-01, -4.4074e-01,\n",
      "          9.8782e-02,  3.1161e-01, -9.9475e-02,  4.2332e-01, -8.8575e-02,\n",
      "          5.1904e-01, -2.7536e-01, -2.0713e-01, -4.9387e-01,  4.6778e-01,\n",
      "         -9.9043e-02],\n",
      "        [ 1.0811e-01,  3.8720e-01, -1.7007e-01, -1.7185e-01,  3.4866e-01,\n",
      "          2.9365e-02, -4.0996e-01, -2.0941e-01, -5.4547e-01,  5.3147e-02,\n",
      "         -4.7515e-01, -1.7661e-01,  2.5679e-01,  4.8605e-01,  1.2113e-02,\n",
      "          4.8272e-01],\n",
      "        [ 3.7029e-01, -3.2598e-02, -1.3674e-01,  5.8303e-02,  1.2173e-01,\n",
      "          3.0116e-01,  3.0896e-01, -8.4508e-02, -4.7731e-01, -7.4532e-03,\n",
      "         -2.7879e-01,  1.0256e-01, -2.5444e-01, -1.6433e-01,  6.4782e-02,\n",
      "          1.3113e-01],\n",
      "        [ 1.0518e-01,  6.3825e-01, -1.8887e-01, -9.4037e-02,  7.6774e-02,\n",
      "          6.0972e-01, -3.6624e-01, -5.2480e-01, -4.8954e-01, -3.5718e-02,\n",
      "          2.4195e-03, -1.5405e-01,  6.0233e-02,  4.8384e-01, -2.4334e-01,\n",
      "          1.7268e-01],\n",
      "        [ 1.9000e-01, -3.5554e-01, -2.9754e-01, -2.9977e-01,  1.7797e-01,\n",
      "          2.5400e-01, -4.6057e-01, -1.9050e-01,  2.4277e-01,  1.4116e-01,\n",
      "         -6.2714e-01,  8.4676e-02,  1.2389e-01,  1.2420e-01, -2.4285e-01,\n",
      "          3.0672e-01],\n",
      "        [-2.0634e-01,  4.1691e-04, -6.6209e-02,  3.0297e-01, -6.0640e-01,\n",
      "         -3.9272e-01,  9.0882e-02,  5.8937e-02,  6.1077e-01, -1.5127e-01,\n",
      "          3.5018e-01, -4.1690e-01, -1.0977e-01, -5.8438e-01,  2.6550e-01,\n",
      "         -4.8667e-01],\n",
      "        [-1.6845e-01,  1.6884e-02,  3.5024e-01,  3.8761e-01,  2.2705e-01,\n",
      "         -3.9809e-01,  4.2997e-01,  2.4978e-01,  5.1467e-01, -5.3606e-01,\n",
      "         -1.7250e-01,  4.0497e-01,  8.4578e-02,  2.9748e-01, -1.6215e-01,\n",
      "         -3.1328e-01],\n",
      "        [-5.8448e-01, -4.7961e-01,  4.4918e-01,  6.1200e-01, -8.4946e-02,\n",
      "         -2.2530e-01,  2.6392e-01,  3.0819e-01,  3.4955e-01, -2.2466e-01,\n",
      "         -1.0629e-01,  5.2624e-02, -1.7853e-01,  4.9842e-01,  4.6337e-01,\n",
      "         -5.5308e-01],\n",
      "        [-1.6591e-01, -4.8696e-01, -4.3010e-01,  5.0780e-01, -1.0284e-01,\n",
      "         -3.2308e-01, -2.8230e-02,  2.5442e-01,  3.0991e-01,  5.0568e-01,\n",
      "          4.3213e-01, -2.0799e-01,  3.9296e-01,  7.9254e-02,  3.1803e-01,\n",
      "         -8.2319e-03],\n",
      "        [ 1.6426e-01, -4.2960e-01,  4.9898e-01, -2.7609e-01,  4.2068e-01,\n",
      "          1.4769e-01,  6.7739e-02, -5.5560e-01, -2.7377e-01,  1.5828e-01,\n",
      "         -1.5191e-01,  6.7682e-02, -2.6988e-03, -9.1809e-02, -8.8810e-02,\n",
      "         -4.8051e-01],\n",
      "        [-4.6076e-02,  1.8437e-01, -4.5797e-01,  1.7189e-01, -7.5044e-02,\n",
      "         -4.1120e-01, -1.3081e-01,  9.0496e-02,  1.1914e-02, -1.5405e-01,\n",
      "          9.3407e-02, -1.5096e-02,  1.9507e-01,  5.5798e-01,  2.4620e-01,\n",
      "         -3.2306e-01],\n",
      "        [ 1.3040e-01,  9.5493e-02, -5.0133e-01,  6.0005e-02, -5.2089e-01,\n",
      "          3.9092e-01, -3.2949e-01, -1.9125e-01, -5.3391e-01, -6.0537e-01,\n",
      "          1.0215e-02, -1.5081e-01, -3.3039e-01, -4.8170e-01,  2.4023e-01,\n",
      "          4.2523e-01],\n",
      "        [-3.6136e-01, -1.9574e-01,  2.9488e-01, -3.0147e-02,  1.8038e-01,\n",
      "         -5.6437e-01, -1.0312e-02,  2.8414e-01,  2.8716e-01,  2.6076e-01,\n",
      "          4.6023e-01, -2.9285e-01,  2.6265e-01,  4.9210e-02, -4.1188e-02,\n",
      "         -2.3632e-01],\n",
      "        [ 4.5201e-02,  5.5153e-01,  6.1498e-01, -2.6678e-01,  1.9225e-01,\n",
      "          1.5012e-01, -3.4848e-01, -1.9306e-01, -1.3334e-01, -3.1861e-01,\n",
      "         -5.8194e-01,  3.5291e-01, -5.2241e-02, -1.7575e-01, -2.4338e-01,\n",
      "          4.2121e-01],\n",
      "        [-1.5892e-01, -3.2547e-01,  4.2416e-01,  5.9387e-01,  8.2959e-02,\n",
      "         -1.7470e-01, -2.0500e-01,  2.3405e-01,  7.4305e-02,  2.2295e-01,\n",
      "          3.3195e-01, -8.3461e-02,  3.3806e-01,  5.1294e-01, -2.9213e-02,\n",
      "         -5.6675e-01],\n",
      "        [-2.2668e-01, -4.5828e-01,  5.3024e-01,  5.0810e-01,  5.2453e-01,\n",
      "         -5.2241e-01,  3.1073e-01,  1.4491e-01,  1.4367e-01,  2.7112e-01,\n",
      "          1.7509e-01, -2.0835e-01,  2.3061e-01,  6.2280e-01,  1.2365e-01,\n",
      "         -4.5191e-01],\n",
      "        [-2.2827e-01,  4.9845e-01, -5.5247e-01,  4.9602e-01,  5.5352e-01,\n",
      "         -1.5290e-01,  3.4091e-01, -4.1993e-01, -4.1599e-01, -1.8942e-02,\n",
      "          6.1021e-01,  1.7191e-01,  9.7037e-03,  8.0835e-02, -3.5504e-01,\n",
      "          1.5350e-01],\n",
      "        [-8.9601e-02, -4.0832e-01,  6.2195e-01,  3.6104e-01,  5.7786e-01,\n",
      "         -3.8078e-01, -3.1084e-01,  2.7671e-01,  1.4402e-01,  4.6870e-01,\n",
      "         -2.3986e-01, -1.1182e-01,  1.4974e-01,  2.2358e-01, -9.6969e-02,\n",
      "         -3.3496e-01],\n",
      "        [ 3.3441e-01, -1.5923e-01,  4.7633e-01, -5.0454e-02, -4.1064e-01,\n",
      "          4.1243e-01, -2.7343e-01,  6.1895e-01, -6.7254e-03, -7.1171e-02,\n",
      "         -1.2418e-01, -1.6204e-01, -1.3215e-02, -2.8549e-01,  2.4200e-01,\n",
      "         -3.9167e-01],\n",
      "        [-2.0506e-01,  5.7003e-02, -3.7384e-01,  3.5729e-01,  8.4076e-02,\n",
      "         -5.8768e-01,  5.7631e-01, -4.4886e-02,  5.4933e-01, -3.0308e-01,\n",
      "         -4.3737e-02,  2.7083e-01,  2.0747e-01, -3.3484e-01, -3.6523e-01,\n",
      "          2.5168e-01],\n",
      "        [ 5.1200e-01, -6.2686e-01,  3.8507e-01, -5.9517e-01, -3.5786e-01,\n",
      "          3.9564e-02, -1.0874e-01,  1.6670e-01,  8.7093e-02,  1.1068e-01,\n",
      "         -1.1171e-01, -7.0388e-02,  1.3456e-01, -2.3725e-01,  9.3745e-02,\n",
      "         -3.1938e-01],\n",
      "        [-4.1080e-01,  5.6269e-01, -4.5881e-01, -2.6437e-02, -1.7678e-01,\n",
      "         -5.1139e-01,  1.4649e-01, -4.0105e-01, -2.5920e-01,  2.3328e-02,\n",
      "          2.2884e-01, -7.8180e-02, -6.2861e-02, -1.3438e-01, -8.0960e-02,\n",
      "         -2.3193e-01],\n",
      "        [ 3.6142e-01,  5.0387e-01, -6.1894e-02, -5.9344e-01, -2.0439e-01,\n",
      "         -2.5157e-01,  3.8733e-01,  1.5999e-02,  3.0269e-01, -5.3911e-01,\n",
      "          9.8703e-02,  2.0073e-01,  1.5485e-01, -4.8406e-01,  1.0396e-01,\n",
      "          3.3535e-01],\n",
      "        [-4.0808e-01,  5.5822e-01, -2.3560e-01, -3.7805e-01,  6.4831e-02,\n",
      "         -1.1232e-01, -1.8900e-01, -5.3327e-01,  2.9300e-02,  1.5776e-01,\n",
      "          4.3839e-01, -2.2057e-01, -2.2027e-01, -5.2481e-01, -2.7703e-01,\n",
      "          3.6350e-01],\n",
      "        [-5.3536e-02, -4.1128e-01, -2.0735e-01, -3.1363e-01, -8.8020e-02,\n",
      "          6.5268e-02, -2.0916e-02,  1.6628e-02,  2.1902e-01, -3.8712e-01,\n",
      "          8.3861e-02,  2.0041e-01,  5.3120e-01,  7.1440e-02,  2.6587e-01,\n",
      "          4.5629e-01],\n",
      "        [ 4.5828e-01, -2.5221e-01, -3.8352e-01, -1.1255e-01, -2.9274e-01,\n",
      "          3.1958e-01, -4.4566e-01,  1.6345e-01,  4.3671e-02, -2.8119e-01,\n",
      "         -3.3539e-01, -1.7808e-01,  3.1532e-02,  3.9000e-01,  2.6787e-01,\n",
      "          1.7029e-01],\n",
      "        [ 2.4637e-01, -4.4176e-01,  4.8612e-01, -7.7482e-02, -1.2299e-01,\n",
      "          1.4135e-01,  1.5838e-01, -8.7457e-02,  6.9154e-02, -7.7980e-02,\n",
      "          1.9128e-02,  5.5589e-01, -3.6003e-01,  3.3897e-01, -2.7860e-01,\n",
      "          1.8201e-01],\n",
      "        [ 2.3082e-01,  3.5888e-01,  2.4492e-01, -1.7782e-01,  4.8408e-02,\n",
      "         -5.8796e-02, -4.2066e-01, -2.0749e-02,  1.0711e-01,  6.1050e-01,\n",
      "          8.7944e-02, -3.8593e-01,  1.6433e-01, -4.4040e-01,  7.7530e-02,\n",
      "         -4.8461e-01],\n",
      "        [ 5.8104e-02, -4.8107e-01,  1.1805e-01, -3.6025e-01,  3.9492e-02,\n",
      "         -1.2885e-01,  2.9215e-01, -1.4627e-01, -3.8507e-01, -6.5158e-02,\n",
      "          4.3860e-02,  9.0917e-02, -1.7098e-01,  4.8931e-01, -2.0383e-01,\n",
      "         -2.1197e-02],\n",
      "        [-4.1389e-02,  9.7691e-02,  1.6871e-01,  6.4612e-02,  2.4903e-01,\n",
      "         -2.6874e-02,  1.2509e-01, -1.6675e-01, -4.7754e-01,  5.6989e-01,\n",
      "         -3.5939e-01,  1.9322e-01, -3.4726e-02, -5.2929e-01, -4.5682e-01,\n",
      "         -1.7803e-01],\n",
      "        [ 2.7141e-01,  5.4940e-01,  1.0864e-01,  7.0667e-02,  1.7091e-02,\n",
      "          9.8744e-02, -6.3169e-02, -2.7600e-01, -4.4948e-02,  5.3832e-02,\n",
      "         -8.7713e-02, -2.3034e-01, -1.5111e-01, -3.3527e-01,  3.6008e-02,\n",
      "         -8.7740e-02],\n",
      "        [-9.5149e-03,  4.6427e-01,  1.4476e-01, -2.4039e-01, -1.9697e-01,\n",
      "          4.6233e-01,  1.5235e-01, -1.2624e-01,  3.8705e-01, -4.1170e-02,\n",
      "         -1.8441e-01, -2.0971e-01, -2.8481e-01, -3.3455e-02,  2.0510e-01,\n",
      "         -6.0787e-01],\n",
      "        [-2.6185e-02, -6.0026e-01,  3.2427e-01,  1.2882e-01, -9.7721e-02,\n",
      "          1.2267e-01,  2.9185e-02,  4.0521e-02,  4.3272e-02, -4.4917e-01,\n",
      "         -9.6585e-02,  4.7661e-01, -2.2527e-01,  1.6200e-01, -5.5225e-04,\n",
      "          4.7727e-01],\n",
      "        [ 1.6960e-01,  1.6094e-01, -4.9534e-01, -6.1812e-02, -2.8883e-01,\n",
      "          3.8353e-01,  1.5961e-02,  3.8777e-01,  2.3041e-01, -5.6614e-03,\n",
      "          1.2434e-02, -5.1057e-01,  2.2794e-01, -7.3018e-02,  5.4358e-02,\n",
      "          2.5499e-01],\n",
      "        [-3.1504e-01,  3.0578e-01,  1.4769e-01,  9.2367e-02,  1.8484e-01,\n",
      "         -3.1267e-01,  1.9582e-01, -4.7689e-01, -1.1161e-01, -6.2210e-02,\n",
      "          4.0278e-01,  4.2615e-01, -2.5298e-01,  1.2963e-01, -1.3960e-01,\n",
      "         -1.4664e-01],\n",
      "        [-1.2750e-01, -2.1196e-01,  2.6400e-01, -5.3625e-02, -2.9226e-01,\n",
      "          2.1202e-02,  5.1495e-01, -3.4220e-01,  3.5326e-01, -2.3704e-01,\n",
      "         -2.0980e-03,  3.4241e-01,  6.8825e-02,  7.6820e-02, -1.1321e-01,\n",
      "          3.9854e-01],\n",
      "        [-2.5050e-01,  3.7214e-02, -4.2446e-01,  3.4844e-01,  2.6068e-01,\n",
      "         -3.4614e-01, -5.5981e-01,  2.3908e-01, -2.0435e-01,  4.8134e-01,\n",
      "         -1.8319e-01, -5.6996e-01,  1.6854e-01, -4.8276e-01,  5.7065e-01,\n",
      "         -3.9047e-01],\n",
      "        [ 2.4900e-01, -3.0191e-01,  5.0173e-01, -4.5676e-01, -6.3984e-02,\n",
      "          2.4812e-01,  4.3665e-01, -3.3289e-01,  1.9150e-02, -2.2958e-01,\n",
      "         -5.7978e-01,  3.3334e-01, -5.7873e-02,  4.4496e-01, -2.1911e-01,\n",
      "          3.5686e-01],\n",
      "        [-2.0380e-02,  1.2709e-01,  3.7552e-01,  1.8321e-01, -1.3515e-01,\n",
      "         -5.3897e-02, -1.4643e-01, -2.7919e-01,  6.7730e-02,  6.1720e-01,\n",
      "         -2.4486e-01,  5.3165e-02, -2.0600e-01, -4.5202e-01,  2.0258e-02,\n",
      "         -3.1565e-01],\n",
      "        [-3.5710e-01,  4.5126e-01, -1.9171e-01,  1.6820e-01,  3.9277e-02,\n",
      "         -1.3679e-01,  2.5801e-01,  2.3415e-01, -4.8997e-01,  2.9231e-01,\n",
      "          1.7105e-01,  8.6796e-02,  6.2741e-02, -6.3243e-01,  1.6901e-02,\n",
      "         -2.0219e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3414,  0.3408, -0.3370,  0.3396, -0.3405, -0.0058, -0.0352,  0.3403,\n",
      "         0.2844, -0.3415, -0.1831,  0.2557, -0.3403,  0.3402, -0.1764, -0.3406,\n",
      "        -0.0085, -0.0160,  0.1024, -0.0238, -0.0094, -0.0044, -0.0791, -0.0543,\n",
      "        -0.0147, -0.0490,  0.0295,  0.0149,  0.0200, -0.0531,  0.0609,  0.0274,\n",
      "         0.0286, -0.0608,  0.3405, -0.3097,  0.3404, -0.2485, -0.3043, -0.2714,\n",
      "         0.3403, -0.2265,  0.2783,  0.3402, -0.3401,  0.3404, -0.3053, -0.2429],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.9609e-01,  3.6787e-01,  1.5818e-03, -5.0325e-02, -1.3951e-01,\n",
      "          1.3307e-01, -1.3997e-01,  2.4246e-01, -1.5420e-01, -2.4687e-01,\n",
      "         -1.9973e-01, -2.0123e-01,  1.4197e-01, -1.1723e-01,  2.6500e-01,\n",
      "          5.1976e-01],\n",
      "        [-2.1614e-01, -7.1932e-02,  2.5721e-02,  4.4534e-01, -1.1065e-03,\n",
      "          1.6676e-01,  1.2831e-01,  1.3609e-01, -5.8786e-02, -7.1149e-02,\n",
      "          1.8199e-01,  9.9746e-03,  1.4974e-01, -4.2164e-01,  1.1167e-01,\n",
      "         -1.5146e-02],\n",
      "        [ 1.6067e-01,  2.3252e-01,  3.1487e-01, -2.8472e-03, -3.7756e-02,\n",
      "         -2.0556e-01, -4.8850e-01,  3.0663e-02,  3.2535e-01, -1.4264e-01,\n",
      "         -3.9334e-01,  3.6075e-01, -1.4457e-01,  9.9327e-02, -1.7082e-01,\n",
      "         -3.4524e-04],\n",
      "        [-1.8262e-01,  2.8253e-02, -1.5742e-04,  2.1104e-01,  1.4702e-01,\n",
      "          9.7979e-02,  1.3082e-01,  1.7416e-01, -2.1880e-01, -1.5666e-01,\n",
      "          3.2569e-01, -1.0101e-01,  2.3595e-01, -3.0798e-01, -1.3148e-02,\n",
      "          1.7885e-01],\n",
      "        [ 2.2033e-01,  3.5861e-01,  4.0623e-01, -7.3776e-02, -1.2338e-01,\n",
      "         -3.1361e-01, -4.1518e-01, -3.1734e-01, -4.6551e-02, -1.0191e-01,\n",
      "         -9.0438e-02,  4.1412e-01, -1.8806e-01,  4.8764e-01, -1.3948e-01,\n",
      "         -2.7940e-01],\n",
      "        [-2.8708e-01, -1.3511e-01, -4.6578e-02,  3.6603e-02, -2.9144e-02,\n",
      "          6.9459e-02, -9.2847e-02,  2.2203e-01, -3.1497e-01, -4.7836e-01,\n",
      "          1.9309e-01,  1.6659e-01,  1.0122e-01,  1.4733e-01, -6.4242e-02,\n",
      "          3.8339e-01],\n",
      "        [-4.6119e-01, -1.3850e-01,  1.2590e-01,  4.5856e-01,  2.4142e-01,\n",
      "          2.3790e-01, -4.2353e-01,  2.5431e-01, -4.3303e-01, -1.7652e-01,\n",
      "          1.5958e-01,  1.3929e-02,  5.9071e-01, -4.3120e-01,  2.8864e-01,\n",
      "          3.8214e-01],\n",
      "        [ 4.0366e-01,  1.9193e-01,  4.1744e-01, -9.8773e-02, -2.9825e-01,\n",
      "         -3.4971e-01, -2.0220e-01, -2.1572e-01,  1.1543e-01, -1.0121e-01,\n",
      "         -4.5813e-01,  2.7304e-01, -3.5102e-01,  1.8138e-01, -3.1412e-01,\n",
      "          1.5637e-01],\n",
      "        [ 2.3650e-01, -2.4109e-01, -3.0652e-01, -3.3621e-01, -2.1974e-01,\n",
      "         -5.7177e-01,  1.3287e-01, -4.8283e-01, -5.4915e-02,  5.4011e-01,\n",
      "          1.5048e-01, -2.0821e-01, -5.7810e-01,  3.9498e-01, -2.1930e-01,\n",
      "          3.7694e-01],\n",
      "        [-4.3877e-01, -1.0784e-03, -1.0537e-01,  7.7747e-02,  2.3760e-01,\n",
      "          3.8584e-01, -2.1681e-01,  1.2974e-01, -3.5694e-01,  1.7122e-01,\n",
      "          3.8464e-01, -2.4788e-01,  2.9736e-01, -5.6421e-01,  2.8399e-01,\n",
      "          1.5719e-01],\n",
      "        [-2.1764e-01, -3.0264e-01,  2.4146e-01, -2.5041e-01,  3.9433e-02,\n",
      "         -3.7790e-01,  1.8650e-02, -3.1030e-01,  3.6629e-01, -2.2622e-01,\n",
      "          2.7929e-01,  3.5369e-01, -2.1878e-01, -1.4360e-01,  4.6591e-01,\n",
      "         -2.1898e-01],\n",
      "        [-1.1532e-01, -2.4566e-01, -3.2094e-01, -4.6170e-01, -2.8029e-01,\n",
      "          4.3911e-01,  5.1568e-01, -1.7510e-01, -2.2008e-01,  3.4733e-01,\n",
      "          1.2575e-01, -4.3845e-01,  5.7441e-01, -2.5760e-01, -2.4463e-01,\n",
      "          1.4881e-01],\n",
      "        [ 1.1538e-01,  8.7955e-02,  1.9700e-01, -1.1285e-01, -2.3580e-01,\n",
      "         -3.3038e-01, -1.8118e-01, -4.7312e-01,  4.8298e-01,  3.5294e-01,\n",
      "         -2.1110e-01,  2.0965e-01, -4.7498e-01,  5.2545e-01, -2.6228e-01,\n",
      "         -1.2004e-01],\n",
      "        [ 1.0134e-01, -9.9789e-02, -2.9481e-01, -1.3559e-01, -2.1249e-01,\n",
      "         -3.4043e-01,  1.0738e-01, -4.6024e-01,  1.5717e-01,  2.4538e-01,\n",
      "         -2.8348e-02, -3.5970e-01, -3.0300e-01,  1.8491e-01, -5.0793e-01,\n",
      "         -1.6084e-01],\n",
      "        [ 4.3642e-02, -9.4828e-02, -1.6693e-01, -3.0043e-01, -3.4511e-02,\n",
      "          1.4927e-01,  1.1550e-01, -4.3430e-01, -5.6358e-01,  3.3463e-01,\n",
      "          8.7036e-02, -1.4451e-01,  3.8971e-02, -1.6053e-01, -2.5683e-02,\n",
      "         -1.9190e-01],\n",
      "        [ 6.8129e-02, -2.0008e-01, -7.2090e-02, -4.2087e-01, -1.5063e-01,\n",
      "         -5.5289e-01, -5.2040e-03, -1.4008e-01,  7.5170e-02,  1.8008e-01,\n",
      "         -9.9586e-02, -7.0274e-02, -2.8830e-01,  2.6904e-02,  2.7507e-02,\n",
      "         -3.2478e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3405,  0.0497,  0.3378, -0.3406,  0.2867, -0.2945, -0.3409,  0.1757,\n",
      "         0.3399, -0.3407,  0.3409, -0.3338,  0.3416,  0.0732, -0.1010,  0.3405],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2530,  0.5050,  0.4055,  ...,  0.3328, -0.1656,  0.3776],\n",
      "        [-0.2274,  0.5348, -0.1252,  ..., -0.5594,  0.2030,  0.5447],\n",
      "        [-0.1818, -0.2515,  0.1176,  ...,  0.0446,  0.0839, -0.5827],\n",
      "        ...,\n",
      "        [ 0.2432,  0.4169, -0.4519,  ...,  0.5416, -0.3185,  0.4408],\n",
      "        [-0.4814,  0.3172, -0.3813,  ..., -0.1246,  0.2652,  0.1277],\n",
      "        [ 0.4585, -0.0032, -0.0738,  ...,  0.5164,  0.0634, -0.3002]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3356,  0.2543, -0.1302,  ..., -0.3328,  0.4318, -0.1685],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3234, -0.3455, -0.2867,  ..., -0.3290, -0.3309, -0.2025],\n",
      "        [ 0.3369,  0.3241, -0.2916,  ...,  0.3502,  0.3156,  0.2245],\n",
      "        [ 0.3373,  0.3636, -0.3540,  ...,  0.3692,  0.3241,  0.1633],\n",
      "        ...,\n",
      "        [-0.3476, -0.3392, -0.1995,  ...,  0.3220, -0.3560,  0.2303],\n",
      "        [-0.2803, -0.3154,  0.3388,  ...,  0.3637, -0.3104,  0.3153],\n",
      "        [-0.3509, -0.3318,  0.3148,  ...,  0.3360, -0.3488,  0.3248]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3153,  0.3423, -0.0870, -0.3154, -0.3319, -0.3491, -0.3520,  0.3495,\n",
      "         0.3495, -0.3709,  0.3591, -0.3315,  0.3691, -0.3531,  0.3186,  0.3506],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6650, 1.3140, 0.6590, 1.3421, 1.2468, 1.2172, 0.6598, 1.2403, 1.3406,\n",
      "        0.6592, 1.3410, 1.3359, 1.3425, 0.6757, 1.3407, 0.7145],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3395, -0.0779,  0.3369, -0.3409,  0.3008,  0.1821, -0.3413,  0.1619,\n",
      "         0.3404, -0.3415,  0.3415, -0.3346,  0.3421, -0.1566, -0.2015,  0.3413],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6636, 1.2728, 0.6584, 1.3413, 0.8549, 0.6626, 0.6598, 0.6735, 1.3377,\n",
      "        0.6586, 1.3409, 1.3359, 1.3419, 0.6714, 1.3391, 0.6903],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1027, -0.1016,  0.3374, -0.3351,  0.3009,  0.2321, -0.3414,  0.3327,\n",
      "         0.3404, -0.3418,  0.3412, -0.3357,  0.3420, -0.1761, -0.2500,  0.3414],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7196, 0.7973, 0.7335, 0.7071, 1.0499, 1.3365, 0.8026, 0.6920, 1.0623,\n",
      "        0.6563, 0.6559, 1.3386, 0.7442, 0.7549, 1.3415, 0.6582],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0628, -0.1793, -0.1960, -0.3403, -0.1421, -0.2749, -0.3396,  0.0289,\n",
      "         0.1233, -0.1846, -0.2635, -0.1604,  0.0052, -0.3363,  0.0762, -0.2703],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, tgt_y = next(iter(normal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "            src = src.permute(2,0,1).to(device)\n",
    "            tgt = tgt.permute(2, 0, 1).to(device)\n",
    "            tgt[:,:,0] =torch.randn(tgt.shape[0], tgt.shape[1])\n",
    "            tgt_y = tgt_y.permute(2, 0, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_y[38,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "        src_mask = utils.generate_square_subsequent_mask(\n",
    "                dim1 = tgt.shape[0],\n",
    "                dim2 = src.shape[0]\n",
    "                ).to(device)\n",
    "\n",
    "        tgt_mask = utils.generate_square_subsequent_mask( \n",
    "                  dim1= tgt.shape[0],\n",
    "                  dim2= tgt.shape[0]\n",
    "                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "        temporal_embedding, output = model(\n",
    "          src=src,\n",
    "          tgt=tgt,\n",
    "          src_mask=src_mask,\n",
    "          tgt_mask=tgt_mask\n",
    "          )\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0067, -0.0980, -0.0832, -0.0675, -0.0127, -0.0442, -0.0419,  0.0682],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[38,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_temporal_embedding = get_temporal_embedding(data=normal_data, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99354"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_temporal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 1, 16])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_temporal_embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_18652\\4264421777.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  normal_temporal_embedding = np.asarray(normal_temporal_embedding)\n",
      "C:\\Users\\Raiyan\\AppData\\Local\\Temp\\ipykernel_18652\\4264421777.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  normal_temporal_embedding = np.asarray(normal_temporal_embedding)\n"
     ]
    }
   ],
   "source": [
    "normal_temporal_embedding = np.asarray(normal_temporal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99354,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_temporal_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_data = pd.read_hdf('attack.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_info = attack_data['isAttack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449919,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "2015-12-28 10:00:00    0\n",
       "2015-12-28 10:00:01    0\n",
       "2015-12-28 10:00:02    0\n",
       "2015-12-28 10:00:03    0\n",
       "2015-12-28 10:00:04    0\n",
       "                      ..\n",
       "2016-02-01 14:59:55    0\n",
       "2016-02-01 14:59:56    0\n",
       "2016-02-01 14:59:57    0\n",
       "2016-02-01 14:59:58    0\n",
       "2016-02-01 14:59:59    0\n",
       "Name: isAttack, Length: 449919, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(449919,)\n"
     ]
    }
   ],
   "source": [
    "attack_data = pd.read_hdf(\"attack.h5\")\n",
    "attack_label = attack_data['isAttack'].to_numpy()\n",
    "print(attack_label.shape)\n",
    "\n",
    "attack_target_label = []\n",
    "flag = 0\n",
    "\n",
    "for i in range(0,attack_label.shape[0]-33,5):\n",
    "    temp_list = (attack_label[i:i+25]) \n",
    "    for item in temp_list:\n",
    "        if item == 1:\n",
    "            flag = 1\n",
    "    attack_target_label.append(flag)\n",
    "    flag = 0\n",
    "    \n",
    "attack_target_label = np.array(attack_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89978,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_target_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"newlabel_swat.npz\",attack_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.where(attack_target_label ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10942,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107, 120, 121, 122, 123, 124, 125, 126, 127, 128], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNdg3f/3Uv7gmdrME7xLpjT",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1VBBnJ5Vl_ha3R66u9xCUJhRC_A2S-utX",
     "timestamp": 1662538421634
    },
    {
     "file_id": "13tDtDC22jInZ2hyvr-xyGRSqWsM8zvuZ",
     "timestamp": 1662461350442
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
